{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ef00dc",
   "metadata": {},
   "source": [
    "# Creating a Job Description Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe9a58",
   "metadata": {},
   "source": [
    "## Importing our Greenhouse API Keys & Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79afecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key (same as working test script)\n",
    "api_key = os.getenv('GREENHOUSE_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"‚ùå No API key found!\")\n",
    "else:\n",
    "    print(\"‚úÖ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20664701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Greenhouse jobs and create DataFrame (exact same method that worked)\n",
    "credentials = base64.b64encode(f\"{api_key}:\".encode()).decode()\n",
    "headers = {\"Authorization\": f\"Basic {credentials}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e702b3",
   "metadata": {},
   "source": [
    "## Creating a Dataframe of all job postings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cdd34",
   "metadata": {},
   "source": [
    "### The Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09423715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 246 jobs from CSV\n",
      "üìä DataFrame shape: (246, 18)\n",
      "üìã Columns: ['id', 'name', 'requisition_id', 'notes', 'confidential', 'is_template', 'copied_from_id', 'status', 'created_at', 'opened_at', 'closed_at', 'updated_at', 'departments', 'offices', 'hiring_team', 'openings', 'custom_fields', 'keyed_custom_fields']\n",
      "           id                     name requisition_id  notes  confidential  \\\n",
      "0  4002141005         Senior Recruiter              1    NaN         False   \n",
      "1  4002859005  Recruiting Coordinator               2    NaN         False   \n",
      "2  4002971005  Enterprise Partnerships              4    NaN         False   \n",
      "3  4002972005           Community Lead              5    NaN         False   \n",
      "4  4003052005        Technical Writer               6    NaN         False   \n",
      "\n",
      "   is_template  copied_from_id  status                created_at  \\\n",
      "0        False             NaN  closed  2022-02-23T16:58:29.538Z   \n",
      "1        False    4.002141e+09  closed  2022-03-01T20:51:57.710Z   \n",
      "2        False    4.002859e+09  closed  2022-03-02T16:50:48.090Z   \n",
      "3        False    4.002971e+09  closed  2022-03-02T17:02:09.712Z   \n",
      "4        False    4.002859e+09  closed  2022-03-02T22:54:09.116Z   \n",
      "\n",
      "                  opened_at                 closed_at  \\\n",
      "0  2022-02-23T16:58:30.014Z  2022-04-07T12:59:03.787Z   \n",
      "1  2022-03-01T20:51:58.870Z  2022-05-20T19:34:49.781Z   \n",
      "2  2022-03-02T16:50:49.085Z  2023-03-13T18:12:25.350Z   \n",
      "3  2022-03-02T17:02:10.803Z  2023-01-17T04:34:08.918Z   \n",
      "4  2022-03-02T22:54:10.105Z  2022-09-14T21:22:20.213Z   \n",
      "\n",
      "                 updated_at  \\\n",
      "0  2022-04-07T12:59:03.807Z   \n",
      "1  2022-05-20T19:34:49.795Z   \n",
      "2  2023-03-13T18:12:25.548Z   \n",
      "3  2023-01-17T04:34:09.045Z   \n",
      "4  2022-12-19T23:19:51.285Z   \n",
      "\n",
      "                                         departments  \\\n",
      "0  [{'id': 4002059005, 'name': 'People', 'parent_...   \n",
      "1  [{'id': 4002059005, 'name': 'People', 'parent_...   \n",
      "2  [{'id': 4002061005, 'name': 'Business Operatio...   \n",
      "3  [{'id': 4002061005, 'name': 'Business Operatio...   \n",
      "4  [{'id': 4002065005, 'name': 'Communications', ...   \n",
      "\n",
      "                                             offices  \\\n",
      "0  [{'id': 4000845005, 'name': 'New York or US-ba...   \n",
      "1  [{'id': 4000844005, 'name': 'Uniswap Labs SOHO...   \n",
      "2  [{'id': 4000844005, 'name': 'Uniswap Labs SOHO...   \n",
      "3  [{'id': 4000844005, 'name': 'Uniswap Labs SOHO...   \n",
      "4  [{'id': 4000845005, 'name': 'New York or US-ba...   \n",
      "\n",
      "                                         hiring_team  \\\n",
      "0  {'hiring_managers': [{'id': 4002852005, 'first...   \n",
      "1  {'hiring_managers': [{'id': 4002852005, 'first...   \n",
      "2  {'hiring_managers': [{'id': 4002852005, 'first...   \n",
      "3  {'hiring_managers': [{'id': 4005367005, 'first...   \n",
      "4  {'hiring_managers': [{'id': 4005364005, 'first...   \n",
      "\n",
      "                                            openings  \\\n",
      "0  [{'id': 4004120005, 'opening_id': None, 'statu...   \n",
      "1  [{'id': 4005008005, 'opening_id': None, 'statu...   \n",
      "2  [{'id': 4005161005, 'opening_id': None, 'statu...   \n",
      "3  [{'id': 4005162005, 'opening_id': None, 'statu...   \n",
      "4  [{'id': 4005250005, 'opening_id': None, 'statu...   \n",
      "\n",
      "                                       custom_fields  \\\n",
      "0  {'employment_type': 'Full-time', 'sourcer': None}   \n",
      "1  {'employment_type': 'Full-time', 'sourcer': None}   \n",
      "2  {'employment_type': 'Full-time', 'sourcer': None}   \n",
      "3  {'employment_type': 'Full-time', 'sourcer': None}   \n",
      "4  {'employment_type': 'Full-time', 'sourcer': None}   \n",
      "\n",
      "                                 keyed_custom_fields  \n",
      "0  {'employment_type': {'name': 'Employment Type'...  \n",
      "1  {'employment_type': {'name': 'Employment Type'...  \n",
      "2  {'employment_type': {'name': 'Employment Type'...  \n",
      "3  {'employment_type': {'name': 'Employment Type'...  \n",
      "4  {'employment_type': {'name': 'Employment Type'...  \n"
     ]
    }
   ],
   "source": [
    "# Check if CSV exists first\n",
    "csv_file = 'data/greenhouse_jobs.csv'\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    # Load from CSV\n",
    "    jobs_df = pd.read_csv(csv_file)\n",
    "    print(f\"‚úÖ Loaded {len(jobs_df)} jobs from CSV\")\n",
    "else:\n",
    "    # Make API call\n",
    "    job_postings_params = {\n",
    "        \"per_page\": 500,\n",
    "        \"page\": 1,\n",
    "        \"full_content\": True,\n",
    "        \"internal\": False\n",
    "    }\n",
    "    \n",
    "    job_postings = requests.get('https://harvest.greenhouse.io/v1/jobs',\n",
    "                               headers=headers, params=job_postings_params)\n",
    "    \n",
    "    print(f\"Status: {job_postings.status_code}\")\n",
    "    \n",
    "    if job_postings.status_code == 200:\n",
    "        jobs = job_postings.json()\n",
    "        jobs_df = pd.DataFrame(jobs)\n",
    "        \n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(csv_file), exist_ok=True)\n",
    "\n",
    "        # Save to CSV for next time\n",
    "        jobs_df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        print(f\"üéâ Success! Found {len(jobs_df)} jobs\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {job_postings.status_code}\")\n",
    "        print(f\"Response: {job_postings.text[:200]}\")\n",
    "        jobs_df = None\n",
    "\n",
    "# Continue with your existing DataFrame operations\n",
    "if jobs_df is not None:\n",
    "    print(f\"üìä DataFrame shape: {jobs_df.shape}\")\n",
    "    print(f\"üìã Columns: {list(jobs_df.columns)}\")\n",
    "    print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0faab3a",
   "metadata": {},
   "source": [
    "### If importing from CSV, run code below\n",
    "- Parses complex nested columns back from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f379aeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed departments\n",
      "‚úÖ Parsed offices\n",
      "‚úÖ Parsed hiring_team\n",
      "‚úÖ Parsed openings\n",
      "‚úÖ Parsed custom_fields\n",
      "‚úÖ Parsed keyed_custom_fields\n",
      "\n",
      "Now departments is a proper list:\n",
      "Type: <class 'list'>\n",
      "First department: {'id': 4002059005, 'name': 'People', 'parent_id': None, 'parent_department_external_id': None, 'child_ids': [], 'child_department_external_ids': [], 'external_id': None}\n"
     ]
    }
   ],
   "source": [
    "# (Only run this if you need to work with departments, offices, etc. as actual lists/dicts)\n",
    "\n",
    "def safe_eval(x):\n",
    "    \"\"\"Safely convert string representation back to Python object\"\"\"\n",
    "    if pd.isna(x) or x == 'nan':\n",
    "        return None\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "# Parse complex columns back to their original structure\n",
    "complex_columns = ['departments', 'offices', 'hiring_team', 'openings', 'custom_fields', 'keyed_custom_fields']\n",
    "\n",
    "for col in complex_columns:\n",
    "    if col in jobs_df.columns:\n",
    "        jobs_df[col] = jobs_df[col].apply(safe_eval)\n",
    "        print(f\"‚úÖ Parsed {col}\")\n",
    "\n",
    "print(\"\\nNow departments is a proper list:\")\n",
    "print(f\"Type: {type(jobs_df.iloc[0]['departments'])}\")\n",
    "if jobs_df.iloc[0]['departments']:\n",
    "    print(f\"First department: {jobs_df.iloc[0]['departments'][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c660f",
   "metadata": {},
   "source": [
    "### Figuring out Unique Departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57d58a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jobs: 246\n",
      "                   department_name  job_count\n",
      "0                      Engineering         87\n",
      "1   Business Operations & Strategy         26\n",
      "2                           People         25\n",
      "3               Product Management         21\n",
      "4              Customer Experience         21\n",
      "5                   Legal & Policy         17\n",
      "6             Business Development         13\n",
      "7                   Communications         12\n",
      "8                         Research          9\n",
      "9                        Marketing          6\n",
      "10                    Data Science          4\n",
      "11                          Design          3\n",
      "12                            Test          2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Extract all department names from the departments column\n",
    "all_departments = []\n",
    "\n",
    "for dept_list in jobs_df['departments']:\n",
    "    if dept_list:  # Check if the list is not empty\n",
    "        for dept in dept_list:\n",
    "            if 'name' in dept:\n",
    "                all_departments.append(dept['name'])\n",
    "\n",
    "# Count frequencies\n",
    "department_counts = Counter(all_departments)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "departments_df = pd.DataFrame(department_counts.items(), columns=['department_name', 'job_count'])\n",
    "\n",
    "# Optional: sort by job count descending\n",
    "departments_df = departments_df.sort_values(by='job_count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "total_jobs = len(jobs_df)\n",
    "print(f\"Total jobs: {total_jobs}\")\n",
    "print(departments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e3094",
   "metadata": {},
   "source": [
    "### Indexing for 'real' roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "986f0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36f916",
   "metadata": {},
   "source": [
    "## Pulling the Job Description from Job_Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba058ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "üéâ Success! Found 401 job posts\n",
      "üìä DataFrame shape: (401, 15)\n",
      "üìã Columns: ['id', 'active', 'live', 'first_published_at', 'title', 'location', 'internal', 'external', 'job_id', 'content', 'internal_content', 'updated_at', 'created_at', 'demographic_question_set_id', 'questions']\n"
     ]
    }
   ],
   "source": [
    "# Check if job posts CSV exists first\n",
    "posts_csv_file = 'data/all_greenhouse_job_posts.csv'\n",
    "\n",
    "if os.path.exists(posts_csv_file):\n",
    "    # Load from CSV\n",
    "    postings_df = pd.read_csv(posts_csv_file)\n",
    "    print(f\"‚úÖ Loaded {len(postings_df)} job posts from CSV\")\n",
    "else:\n",
    "    # Make API call\n",
    "    post_params = {\n",
    "        \"per_page\": 500,\n",
    "        \"page\": 1\n",
    "        # \"active\": True,  # Omit this to get all job posts\n",
    "        # \"full_content\": True # This is the full job description\n",
    "    }\n",
    "    # GET: List Job Posts \n",
    "    job_posts = requests.get('https://harvest.greenhouse.io/v1/job_posts',\n",
    "                            headers=headers, params=post_params)\n",
    "    \n",
    "    print(f\"Status: {job_posts.status_code}\")\n",
    "    \n",
    "    if job_posts.status_code == 200:\n",
    "        postings = job_posts.json()\n",
    "        postings_df = pd.DataFrame(postings)\n",
    "        \n",
    "        # Save to CSV for next time\n",
    "        postings_df.to_csv(posts_csv_file, index=False)\n",
    "        \n",
    "        print(f\"üéâ Success! Found {len(postings_df)} job posts\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {job_posts.status_code}\")\n",
    "        print(f\"Response: {job_posts.text[:200]}\")\n",
    "        postings_df = None\n",
    "\n",
    "# Continue with your DataFrame operations\n",
    "if postings_df is not None:\n",
    "    print(f\"üìä DataFrame shape: {postings_df.shape}\")\n",
    "    print(f\"üìã Columns: {list(postings_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e414f2",
   "metadata": {},
   "source": [
    "### Filtering by 'content' for JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c4549bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST JOB DESCRIPTION ===\n",
      "<p>As we enter into this next phase of growth, we are looking to more than double our team and implement creative talent processes and strategies that can preserve and grow our culture of innovation. We are looking to make our first talent acquisition hire to help us scale and build out a world-class team. While the majority of our hiring will be for technical roles, we are looking for a technical recruiter to help us build a leading and diverse engineering team.</p>\n",
      "<h3>Responsibilities:</h3>\n",
      "<ul>\n",
      "<li>Develop overall talent acquisition strategy</li>\n",
      "<li>Build out processes and tools necessary to manage TA at scale</li>\n",
      "<li>End-to-end recruitment from sourcing through to close</li>\n",
      "<li>Understand business goal and manage internal stakeholders to drive outcomes</li>\n",
      "<li>Partner with hiring managers to develop and execute systematic hiring process across all searches</li>\n",
      "<li>Develop and maintain an excellent candidate experience</li>\n",
      "<li>Identify, source, and interview candidates</li>\n",
      "<li>Manage recruiting support team</li>\n",
      "</ul>\n",
      "<h3>Requirements:</h3>\n",
      "<ul>\n",
      "<li>2+ years of talent acquisition at a high growth company</li>\n",
      "<li>Experience recruiting for technical and non technical positions</li>\n",
      "<li>Ability to think strategically about how to achieve goals and the humility and execution orientation necessary to achieve them</li>\n",
      "<li>Equally comfortable sourcing candidates as you interviewing and building relationships with them</li>\n",
      "<li>Team management experience desired but not required</li>\n",
      "<li>Passion for crypto and decentralized finance</li>\n",
      "</ul>\n",
      "<h3>Nice to Haves:</h3>\n",
      "<ul>\n",
      "<li>Love for unicorns &lt;3</li>\n",
      "</ul>\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the job description\n",
    "job_description = postings_df['content']\n",
    "\n",
    "# Method 1: See specific job descriptions (best for reading)\n",
    "print(\"=== FIRST JOB DESCRIPTION ===\")\n",
    "print(job_description.iloc[0])  # First job\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb97b2",
   "metadata": {},
   "source": [
    "#### Cleaning the JD\n",
    "- Removing html headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5841bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CLEAN TEXT DESCRIPTIONS ===\n",
      "\n",
      "Title: Senior Recruiter\n",
      "Clean Description: As we enter into this next phase of growth, we are looking to more than double our team and implement creative talent processes and strategies that can preserve and grow our culture of innovation. We are looking to make our first talent acquisition hire to help us scale and build out a world-class team. While the majority of our hiring will be for technical roles, we are looking for a technical recruiter to help us build a leading and diverse engineering team.\n",
      "Responsibilities:\n",
      "\n",
      "Develop overall talent acquisition strategy\n",
      "Build out processes and tools necessary to manage TA at scale\n",
      "End-to-end recruitment from sourcing through to close\n",
      "Understand business goal and manage internal stakeholders to drive outcomes\n",
      "Partner with hiring managers to develop and execute systematic hiring process across all searches\n",
      "Develop and maintain an excellent candidate experience\n",
      "Identify, source, and interview candidates\n",
      "Manage recruiting support team\n",
      "\n",
      "Requirements:\n",
      "\n",
      "2+ years of talent acquisition at a high growth company\n",
      "Experience recruiting for technical and non technical positions\n",
      "Ability to think strategically about how to achieve goals and the humility and execution orientation necessary to achieve them\n",
      "Equally comfortable sourcing candidates as you interviewing and building relationships with them\n",
      "Team management experience desired but not required\n",
      "Passion for crypto and decentralized finance\n",
      "\n",
      "Nice to Haves:\n",
      "\n",
      "Love for unicorns &lt;3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "def clean_html(text):\n",
    "    \"\"\"Remove HTML tags from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Remove HTML tags\n",
    "    clean = re.sub('<.*?>', '', str(text))\n",
    "    # Replace common HTML entities\n",
    "    clean = clean.replace('&nbsp;', ' ').replace('&amp;', '&')\n",
    "    return clean.strip()\n",
    "\n",
    "print(\"\\n=== CLEAN TEXT DESCRIPTIONS ===\")\n",
    "# for i in range(min(3, len(postings_df))):\n",
    "title = postings_df.iloc[0].get('title', 'No title')\n",
    "content = clean_html(postings_df.iloc[0]['content'])\n",
    "    \n",
    "print(f\"\\nTitle: {title}\")\n",
    "print(f\"Clean Description: {content}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956e571",
   "metadata": {},
   "source": [
    "## Creating a condensed DF of job id, title, department, & JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7771817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating cleaned dataset...\n",
      "üìä Before deduplication: 401 job posts\n",
      "üìä Unique job_ids: 236\n",
      "üìä After deduplication: 236 job posts\n",
      "‚úÖ Created dataset with 236 job postings\n",
      "üìä Columns: ['job_id', 'job_department', 'job_title', 'job_description']\n",
      "üìà Departments: job_department\n",
      "Engineering                       82\n",
      "People                            25\n",
      "Business Operations & Strategy    25\n",
      "Product Management                20\n",
      "Customer Experience               19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the cleaned_job_dataset.csv exists\n",
    "\n",
    "if os.path.exists('data/cleaned_job_dataset.csv'):\n",
    "    print(\"‚úÖ Cleaned dataset already exists, loading from file...\")\n",
    "    job_dataset = pd.read_csv('data/cleaned_job_dataset.csv')\n",
    "    print(f\"üìä Loaded {len(job_dataset)} job postings\")\n",
    "    print(f\"üìà Departments: {job_dataset['job_department'].value_counts().head()}\")\n",
    "\n",
    "else:\n",
    "    print(\"üîÑ Creating cleaned dataset...\")\n",
    "\n",
    "    # Load the data\n",
    "    jobs_df = pd.read_csv('data/greenhouse_jobs.csv')\n",
    "    posts_df = pd.read_csv('data/all_greenhouse_job_posts.csv')\n",
    "\n",
    "    # Extract department name from departments field\n",
    "    def get_department_name(dept_str):\n",
    "        if pd.isna(dept_str):\n",
    "            return 'Unknown'\n",
    "        try:\n",
    "            dept_list = ast.literal_eval(dept_str)\n",
    "            return dept_list[0]['name'] if dept_list else 'Unknown'\n",
    "        except:\n",
    "            return 'Unknown'\n",
    "\n",
    "    jobs_df['department_name'] = jobs_df['departments'].apply(get_department_name)\n",
    "\n",
    "    # DEDUPLICATE: Keep only one post per job_id (prefer external, then active posts)\n",
    "    print(f\"üìä Before deduplication: {len(posts_df)} job posts\")\n",
    "    print(f\"üìä Unique job_ids: {posts_df['job_id'].nunique()}\")\n",
    "    \n",
    "    # Sort by preference: external=True, active=True, then by id (newest first)\n",
    "    posts_df_sorted = posts_df.sort_values(['job_id', 'external', 'active', 'id'], \n",
    "                                          ascending=[True, False, False, False])\n",
    "    \n",
    "    # Keep only the first (best) post per job_id\n",
    "    posts_df_unique = posts_df_sorted.drop_duplicates(subset=['job_id'], keep='first')\n",
    "    \n",
    "    print(f\"üìä After deduplication: {len(posts_df_unique)} job posts\")\n",
    "\n",
    "    # Merge dataframes\n",
    "    merged_df = posts_df_unique.merge(jobs_df[['id', 'department_name']], \n",
    "                                     left_on='job_id', \n",
    "                                     right_on='id', \n",
    "                                     how='inner')\n",
    "\n",
    "    # Create final dataframe\n",
    "    job_dataset = merged_df[['job_id', 'department_name', 'title', 'content']].rename(columns={\n",
    "        'job_id': 'job_id',\n",
    "        'department_name': 'job_department', \n",
    "        'title': 'job_title',\n",
    "        'content': 'job_description'\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    job_dataset.to_csv('data/cleaned_job_dataset.csv', index=False)\n",
    "\n",
    "    print(f\"‚úÖ Created dataset with {len(job_dataset)} job postings\")\n",
    "    print(f\"üìä Columns: {list(job_dataset.columns)}\")\n",
    "    print(f\"üìà Departments: {job_dataset['job_department'].value_counts().head()}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
