{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Enhancing RAG with Contextual Retrieval\n",
        "\n",
        "This notebook demonstrates contextual retrieval techniques using a modular, clean codebase.\n",
        "\n",
        "## Overview\n",
        "- **Basic RAG**: Standard document chunking and embedding\n",
        "- **Contextual RAG**: Enhanced embeddings with situational context\n",
        "- **Combined RAG**: Multi-document search with source attribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import json\n",
        "from vector_db import VectorDB, ContextualVectorDB\n",
        "from rag_operations import (\n",
        "    retrieve_base, retrieve_contextual, retrieve_combined,\n",
        "    answer_query_base, answer_query_contextual, answer_query_combined\n",
        ")\n",
        "from data_utils import (\n",
        "    transform_data_for_vectordb, create_contextual_dataset, \n",
        "    create_combined_dataset, add_contextual_information\n",
        ")\n",
        "import config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Basic RAG Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading vector database from disk.\n",
            "Loaded 77 chunks into VectorDB\n"
          ]
        }
      ],
      "source": [
        "# Load and transform employee handbook data\n",
        "with open('../data/employee_handbook.json', 'r') as f:\n",
        "    employee_handbook_raw = json.load(f)\n",
        "\n",
        "# Transform data for VectorDB\n",
        "employee_handbook = transform_data_for_vectordb(employee_handbook_raw, \"employee_handbook\")\n",
        "\n",
        "# Initialize and load VectorDB\n",
        "db = VectorDB(\"employee_handbook\")\n",
        "db.load_data(employee_handbook)\n",
        "\n",
        "print(f\"Loaded {len(employee_handbook[0]['chunks'])} chunks into VectorDB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Do we get any sort of wfh-setup-refresh benefits?\n",
            "Answer: Yes, Uniswap Labs provides a Home Office Set up benefit for remote team members. The company reimburses up to $2,000 USD to cover the purchase of office supplies, productivity items, and anything else you might need to get your home office set up.\n",
            "\n",
            "Additionally, if you prefer to work from a co-working space, Uniswap Labs reimburses the cost up to $500 USD per month for co-working space expenses.\n",
            "\n",
            "The company also provides a $50 per month cell phone stipend for benefits-eligible team members that is automatically added to your paychecks. This covers business-related use of personal cell phones, including both services and costs related to their devices. The stipend is non-taxable and is payable for every full month of employment.\n",
            "\n",
            "Beyond the initial home office setup, Uniswap Labs provides you with a company issued computer to do your job.\n",
            "\n",
            "Double-check with Julian or Megan for any of this information!\n"
          ]
        }
      ],
      "source": [
        "# Test basic RAG system\n",
        "test_query = \"Do we get any sort of wfh-setup-refresh benefits?\"\n",
        "answer = answer_query_base(test_query, db)\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Answer: {answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contextual RAG, Creating JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Functions defined for creating contextual employee handbook JSON!\n"
          ]
        }
      ],
      "source": [
        "# Create contextual JSON for Employee Handbook\n",
        "import json\n",
        "from anthropic import Anthropic\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize Anthropic client\n",
        "client = Anthropic()\n",
        "\n",
        "# Template for the full employee handbook document\n",
        "HANDBOOK_DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
        "<document>\n",
        "This is Uniswap Labs' comprehensive Employee Handbook for team members. \n",
        "It covers company policies, employment guidelines, code of conduct, workplace policies, \n",
        "compensation, benefits, and operational procedures.\n",
        "\n",
        "{doc_content}\n",
        "</document>\n",
        "\"\"\"\n",
        "\n",
        "# Template for contextualizing individual chunks\n",
        "HANDBOOK_CHUNK_CONTEXT_PROMPT = \"\"\"\n",
        "Here is a specific section from Uniswap's Employee Handbook that we want to situate within the overall handbook:\n",
        "\n",
        "<chunk>\n",
        "Section: {chunk_heading}\n",
        "{chunk_content}\n",
        "</chunk>\n",
        "\n",
        "Please provide a short, succinct context to situate this handbook section within Uniswap's overall employee policies and procedures. This context will help employees find this information when searching for related policy topics.\n",
        "\n",
        "Answer only with the succinct context and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "def situate_handbook_context(full_handbook_doc: str, chunk_heading: str, chunk_content: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate contextual information for a handbook chunk within the full handbook document\n",
        "    \"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=150,  # Keep context concise\n",
        "        temperature=0.0,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": HANDBOOK_DOCUMENT_CONTEXT_PROMPT.format(doc_content=full_handbook_doc),\n",
        "                        \"cache_control\": {\"type\": \"ephemeral\"}  # Cache the full handbook doc\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": HANDBOOK_CHUNK_CONTEXT_PROMPT.format(\n",
        "                            chunk_heading=chunk_heading,\n",
        "                            chunk_content=chunk_content\n",
        "                        ),\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
        "    )\n",
        "    return response.content[0].text.strip()\n",
        "\n",
        "def add_contextual_information_handbook(input_file, output_file, original_markdown_file):\n",
        "    \"\"\"\n",
        "    Process all employee handbook chunks to add contextual information\n",
        "    \"\"\"\n",
        "    # Load the chunked JSON data\n",
        "    with open(input_file, 'r') as f:\n",
        "        chunks = json.load(f)\n",
        "    \n",
        "    # Load the original full markdown document for context\n",
        "    with open(original_markdown_file, 'r') as f:\n",
        "        full_handbook_doc = f.read()\n",
        "    \n",
        "    print(f\"Processing {len(chunks)} employee handbook chunks...\")\n",
        "    \n",
        "    enhanced_chunks = []\n",
        "    \n",
        "    for chunk in tqdm(chunks, desc=\"Adding contextual information\"):\n",
        "        try:\n",
        "            # Generate situational context\n",
        "            situational_context = situate_handbook_context(\n",
        "                full_handbook_doc=full_handbook_doc,\n",
        "                chunk_heading=chunk['chunk_heading'],\n",
        "                chunk_content=chunk['text']\n",
        "            )\n",
        "            \n",
        "            # Create enhanced chunk\n",
        "            enhanced_chunk = {\n",
        "                \"chunk_link\": chunk[\"chunk_link\"],\n",
        "                \"chunk_heading\": chunk[\"chunk_heading\"],\n",
        "                \"text\": chunk[\"text\"],\n",
        "                \"situational_context\": situational_context\n",
        "            }\n",
        "            \n",
        "            enhanced_chunks.append(enhanced_chunk)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk '{chunk['chunk_heading']}': {e}\")\n",
        "            # Add chunk without context if there's an error\n",
        "            enhanced_chunks.append(chunk)\n",
        "    \n",
        "    # Save the enhanced chunks\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(enhanced_chunks, f, indent=2)\n",
        "    \n",
        "    print(f\"Contextual information added! Enhanced chunks saved to {output_file}\")\n",
        "    return enhanced_chunks\n",
        "\n",
        "print(\"Functions defined for creating contextual employee handbook JSON!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 77 employee handbook chunks with 1 workers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:   1%|‚ñè         | 1/77 [00:02<02:34,  2.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: For our team members working outside of New York State:\n",
            "Situational Context: This section provides important information for Uniswap Labs employees working outside of New York State. It outlines that the handbook covers policies applicable to all Uniswap employees, but also includes specific information for New York-based employees. It clarifies that employees in other states may be subject to different state-specific policies and benefits, and may receive supplemental state-specific handbooks. This section also reiterates Uniswap's at-will employment policy, which applies to all employees regardless of location.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:   3%|‚ñé         | 2/77 [00:04<02:31,  2.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: **Uniswap Principles | Uni-code**\n",
            "Situational Context: This section on \"Uniswap Principles | Uni-code\" outlines Uniswap's core values and guiding principles that shape the company's culture and how employees interact with each other, users, and the broader community. These principles serve as daily guideposts for Uniswap's performance-driven and collaborative work environment, and are foundational to the company's mission of building a people-first, internet-native financial system.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:   4%|‚ñç         | 3/77 [00:05<02:18,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: Code of Ethics {#code-of-ethics}\n",
            "Situational Context: This section on Uniswap's Code of Ethics is part of the \"How We Keep You Safe\" section of the employee handbook, which outlines policies and guidelines related to workplace conduct, ethics, and compliance. It establishes the company's expectations for ethical behavior and confidentiality, and the consequences for violating these policies.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:   5%|‚ñå         | 4/77 [00:07<02:15,  1.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: \n",
            "Situational Context: This section on Compliance with Anti-Corruption and Anti-Money Laundering Laws is part of Uniswap Labs' overall policies and procedures outlined in the Employee Handbook. It provides guidance to employees on Uniswap's commitment to conducting business ethically and in full compliance with applicable laws, including the Foreign Corrupt Practices Act (FCPA) and anti-money laundering regulations. This policy is critical to maintaining Uniswap's reputation for responsible corporate citizenship.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:   6%|‚ñã         | 5/77 [00:09<02:13,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: Devices and Communications {#devices-and-communications}\n",
            "Situational Context: This section on Devices and Communications is part of the \"How We Keep You Safe\" section of Uniswap's Employee Handbook. It outlines the company's policies and guidelines around the use of Uniswap-provided devices, electronic communications, and collaboration tools like Slack and Google Meet. These policies aim to protect the security and confidentiality of Uniswap's information while also providing guidance on appropriate use of the company's technology resources.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:   8%|‚ñä         | 6/77 [00:11<02:06,  1.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: \n",
            "Situational Context: This section on Compliance with Anti-Corruption and Anti-Money Laundering Laws is part of Uniswap Labs' overall policies and procedures outlined in the Employee Handbook. It provides guidance on the company's commitment to conducting business ethically and in compliance with applicable laws, as well as the responsibilities of employees in this regard.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:   9%|‚ñâ         | 7/77 [00:12<01:59,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: \n",
            "Situational Context: This section on Compliance with Anti-Corruption and Anti-Money Laundering Laws is part of Uniswap Labs' overall policies and procedures that outline the company's commitment to conducting business ethically and in full compliance with all applicable laws and regulations. It provides guidance to employees on the company's expectations and requirements around anti-corruption and anti-money laundering practices.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding contextual information:  10%|‚ñà         | 8/77 [00:14<02:04,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: Social Media Guidelines  {#social-media-guidelines}\n",
            "Situational Context: This section on Social Media Guidelines is part of Uniswap's overall employee handbook, which covers company policies, employment guidelines, code of conduct, workplace policies, compensation, benefits, and operational procedures. It provides guidance to Uniswap employees on how to responsibly engage with the company's social media platforms and their own personal accounts, with a focus on maintaining the company's reputation and brand.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Chunk: \n",
            "Situational Context: This section on Compliance with Anti-Corruption and Anti-Money Laundering Laws is part of Uniswap Labs' overall policies and procedures outlined in the Employee Handbook. It provides guidance to employees on Uniswap's commitment to conducting business ethically and in full compliance with applicable laws, including the Foreign Corrupt Practices Act (FCPA) and anti-money laundering regulations. This policy is critical to Uniswap's continued success and reputation as a responsible corporate citizen.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Chunk: Travel and Event Safety {#travel-and-event-safety}\n",
            "Situational Context: This section on Travel and Event Safety is part of the \"How We Keep You Safe\" section of Uniswap's Employee Handbook. It outlines the company's policies and procedures for protecting the physical safety of employees when they participate in travel or attend events outside of the Uniswap office.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "input_file = 'data/employee_handbook.json'\n",
        "output_file = 'data/employee_handbook_with_context.json'\n",
        "original_markdown_file = '../../raw_data_and_data_cleaning/md files/Employee Handbook.md'\n",
        "\n",
        "def add_contextual_information_handbook(input_file, output_file, original_markdown_file, max_workers=1):\n",
        "    \"\"\"\n",
        "    Process all employee handbook chunks to add contextual information in parallel\n",
        "    \"\"\"\n",
        "    # Load the chunked JSON data\n",
        "    with open(input_file, 'r') as f:\n",
        "        chunks = json.load(f)\n",
        "    \n",
        "    # Load the original full markdown document for context\n",
        "    with open(original_markdown_file, 'r') as f:\n",
        "        full_handbook_doc = f.read()\n",
        "    \n",
        "    print(f\"Processing {len(chunks)} employee handbook chunks with {max_workers} workers...\")\n",
        "    \n",
        "    def process_single_chunk(chunk):\n",
        "        try:\n",
        "            # Generate situational context\n",
        "            situational_context = situate_handbook_context(\n",
        "                full_handbook_doc=full_handbook_doc,\n",
        "                chunk_heading=chunk['chunk_heading'],\n",
        "                chunk_content=chunk['text']\n",
        "            )\n",
        "\n",
        "            # Print the chunk info\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Chunk: {chunk['chunk_heading']}\")\n",
        "            print(f\"Situational Context: {situational_context}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            \n",
        "            # Create enhanced chunk\n",
        "            return {\n",
        "                \"chunk_link\": chunk[\"chunk_link\"],\n",
        "                \"chunk_heading\": chunk[\"chunk_heading\"],\n",
        "                \"text\": chunk[\"text\"],\n",
        "                \"situational_context\": situational_context\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk '{chunk['chunk_heading']}': {e}\")\n",
        "            return chunk\n",
        "    \n",
        "    enhanced_chunks = []\n",
        "    \n",
        "    # Process chunks in parallel\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all chunks for processing\n",
        "        future_to_chunk = {executor.submit(process_single_chunk, chunk): chunk for chunk in chunks}\n",
        "        \n",
        "        # Collect results as they complete\n",
        "        for future in tqdm(as_completed(future_to_chunk), total=len(chunks), desc=\"Adding contextual information\"):\n",
        "            result = future.result()\n",
        "            enhanced_chunks.append(result)\n",
        "    \n",
        "    # Save the enhanced chunks\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(enhanced_chunks, f, indent=2)\n",
        "    \n",
        "    print(f\"Contextual information added! Enhanced chunks saved to {output_file}\")\n",
        "    return enhanced_chunks\n",
        "\n",
        "\n",
        "enhanced_chunks = add_contextual_information_handbook(\n",
        "    input_file=input_file,\n",
        "    output_file=output_file,\n",
        "    original_markdown_file=original_markdown_file\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìñ Loading contextual employee handbook data...\n",
            "‚úÖ Loaded 77 contextual chunks\n",
            "üîÑ Transforming data for ContextualVectorDB...\n",
            "üìä Transformed data structure:\n",
            "  - Document ID: employee_handbook_contextual\n",
            "  - Total chunks: 77\n",
            "  - Full content length: 152067 characters\n"
          ]
        }
      ],
      "source": [
        "# Transform contextual JSON to VectorDB format and create pickle file\n",
        "import json\n",
        "import os\n",
        "from vector_db import ContextualVectorDB\n",
        "\n",
        "def transform_contextual_data_for_vectordb(contextual_data: list, doc_id: str = \"employee_handbook_contextual\") -> list:\n",
        "    \"\"\"Transform contextual JSON data to match ContextualVectorDB structure.\"\"\"\n",
        "    \n",
        "    # Reconstruct the full document content for the ContextualVectorDB\n",
        "    full_content = \"\\n\\n\".join([chunk[\"text\"] for chunk in contextual_data])\n",
        "    \n",
        "    doc = {\n",
        "        \"doc_id\": doc_id,\n",
        "        \"original_uuid\": f\"{doc_id}_doc\",\n",
        "        \"content\": full_content,  # Full document content for context generation\n",
        "        \"chunks\": []\n",
        "    }\n",
        "    \n",
        "    for i, item in enumerate(contextual_data):\n",
        "        chunk = {\n",
        "            \"chunk_id\": f\"chunk_{i}\",\n",
        "            \"original_index\": i,\n",
        "            \"content\": item[\"text\"],  # Original content\n",
        "            \"heading\": item[\"chunk_heading\"],\n",
        "            \"link\": item[\"chunk_link\"],\n",
        "            \"contextual_content\": item.get(\"situational_context\", \"\")  # Pre-computed contextual info\n",
        "        }\n",
        "        doc[\"chunks\"].append(chunk)\n",
        "    \n",
        "    return [doc]\n",
        "\n",
        "# Load the contextual JSON data\n",
        "print(\"üìñ Loading contextual employee handbook data...\")\n",
        "with open('data/employee_handbook_with_context.json', 'r') as f:\n",
        "    contextual_data = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(contextual_data)} contextual chunks\")\n",
        "\n",
        "# Transform data for ContextualVectorDB\n",
        "print(\"üîÑ Transforming data for ContextualVectorDB...\")\n",
        "transformed_data = transform_contextual_data_for_vectordb(contextual_data)\n",
        "\n",
        "print(f\"üìä Transformed data structure:\")\n",
        "print(f\"  - Document ID: {transformed_data[0]['doc_id']}\")\n",
        "print(f\"  - Total chunks: {len(transformed_data[0]['chunks'])}\")\n",
        "print(f\"  - Full content length: {len(transformed_data[0]['content'])} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating PreComputed Contextual Vector Database...\n",
            "üìä Loading data into contextual vector database...\n",
            "Loading vector database from disk.\n",
            "üéâ Successfully created contextual vector database pickle file!\n"
          ]
        }
      ],
      "source": [
        "# Create a specialized ContextualVectorDB that uses pre-computed contextual information\n",
        "from vector_db import ContextualVectorDB\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "import pickle\n",
        "import config\n",
        "\n",
        "class PreComputedContextualVectorDB(ContextualVectorDB):\n",
        "    \"\"\"Contextual Vector DB that uses pre-computed contextual information instead of generating it.\"\"\"\n",
        "    \n",
        "    def __init__(self, name: str, openai_api_key: str = None):\n",
        "        # Only initialize OpenAI client, skip Anthropic since we have pre-computed context\n",
        "        if openai_api_key is None:\n",
        "            openai_api_key = config.OPENAI_API_KEY\n",
        "        \n",
        "        self.openai_client = OpenAI(api_key=openai_api_key)\n",
        "        self.name = name\n",
        "        self.embeddings = []\n",
        "        self.metadata = []\n",
        "        self.query_cache = {}\n",
        "        self.db_path = f\"./data/{name}/contextual_vector_db.pkl\"\n",
        "\n",
        "    def load_data_with_precomputed_context(self, dataset: list):\n",
        "        \"\"\"Load data with pre-computed contextual information.\"\"\"\n",
        "        if self.embeddings and self.metadata:\n",
        "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
        "            return\n",
        "        if os.path.exists(self.db_path):\n",
        "            print(\"Loading vector database from disk.\")\n",
        "            self.load_db()\n",
        "            return\n",
        "\n",
        "        texts_to_embed = []\n",
        "        metadata = []\n",
        "        total_chunks = sum(len(doc['chunks']) for doc in dataset)\n",
        "        \n",
        "        print(f\"Processing {total_chunks} chunks with pre-computed contextual information...\")\n",
        "        \n",
        "        with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
        "            for doc in dataset:\n",
        "                for chunk in doc['chunks']:\n",
        "                    # Combine original content with pre-computed contextual information\n",
        "                    contextual_text = f\"{chunk['content']}\\n\\n{chunk['contextual_content']}\"\n",
        "                    \n",
        "                    texts_to_embed.append(contextual_text)\n",
        "                    metadata.append({\n",
        "                        'doc_id': doc['doc_id'],\n",
        "                        'original_uuid': doc['original_uuid'],\n",
        "                        'chunk_id': chunk['chunk_id'],\n",
        "                        'original_index': chunk['original_index'],\n",
        "                        'original_content': chunk['content'],\n",
        "                        'contextual_content': chunk['contextual_content'],\n",
        "                        'heading': chunk.get('heading', ''),\n",
        "                        'link': chunk.get('link', '')\n",
        "                    })\n",
        "                    pbar.update(1)\n",
        "\n",
        "        self._embed_and_store(texts_to_embed, metadata)\n",
        "        self.save_db()\n",
        "        print(f\"‚úÖ Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\")\n",
        "\n",
        "# Create the contextual vector database\n",
        "print(\"üîß Creating PreComputed Contextual Vector Database...\")\n",
        "contextual_db = PreComputedContextualVectorDB(\"employee_handbook_contextual\")\n",
        "\n",
        "# Load the transformed data\n",
        "print(\"üìä Loading data into contextual vector database...\")\n",
        "contextual_db.load_data_with_precomputed_context(transformed_data)\n",
        "\n",
        "print(\"üéâ Successfully created contextual vector database pickle file!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Contextual RAG Setup\n",
        "\n",
        "We'll demonstrate contextual RAG using the Benefits & Wellbeing document. Contextual RAG enhances each chunk with situational context before embedding, leading to better retrieval accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìñ Loading contextual employee handbook database...\n"
          ]
        }
      ],
      "source": [
        "# Example: Full RAG Pipeline - Retrieve + Generate Answer\n",
        "from rag_operations import answer_query_contextual\n",
        "\n",
        "# Load the contextual database\n",
        "print(\"üìñ Loading contextual employee handbook database...\")\n",
        "contextual_handbook_db = PreComputedContextualVectorDB(\"employee_handbook_contextual\")\n",
        "contextual_handbook_db.load_db()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUERY TO ANSWER\n",
            "============================================================\n",
            "\n",
            "Query: What are Uniswap's core values?\n",
            "--------------------------------------------------\n",
            "\n",
            "Answer: Uniswap's core values are articulated through their operating principles called \"Unicode,\" which serve as daily guideposts for how they interact with each other, users, and their community.\n",
            "\n",
            "**People First** emphasizes that Uniswap believes easy, safe, fair value transfer on the internet can improve people's lives, with access, security and experience at the center of everything they do. By pursuing decentralization, interoperability, and durability, they align with users over the long-term. Internally, people are their greatest asset, and they strive for an environment where everyone can make an incredible impact. They share direct, kind feedback for improvement and advocate for ideas by starting with why it's better for users and the company.\n",
            "\n",
            "**Simple** reflects their craft of keeping things simple in a complex field by creating clarity and simplicity. They write and build in human terms that everyone can understand, which shows up in their user experience design, code, and communication with the world and each other. They are deliberate in the words and images they use.\n",
            "\n",
            "**Pink** represents their love of pink both conceptually and in real life, showing that disrupting the status quo is fun. They hold each other accountable to do serious work without taking themselves too seriously. They love unicorns and bring whimsy into experiments and day-to-day work. The people-first, internet-native financial system they're building welcomes anyone.\n",
            "\n",
            "**Push Through Ambiguity Together** acknowledges that startup life can be ambiguous, especially at the frontier, but they are at peace with uncertainty and push through resistance. They show up ready to explore new ideas with conviction that their mission is worth it. When things get hard, they stay on the same team and create scalable solutions to reduce ambiguity. They don't assume shared knowledge across their team or users, seek to understand others' \"why,\" assume positive intent, and default to trust when disagreeing.\n",
            "\n",
            "**Build to last, iterate fast** means they see through today's challenges into tomorrow's possibilities without falling prey to short-term thinking or getting distracted by narratives and prices. They aim to move fast, learn from mistakes, and ship when it's right. They express opinions that bring safety, simplicity, and new choices in users' best interests. This long-term view contributes to their beloved brand status, which they must protect.\n",
            "\n",
            "Double-check with Julian or Megan for any of this information!\n",
            "\n",
            "============================================================\n",
            "UNDERLYING RETRIEVAL DETAILS\n",
            "============================================================\n",
            "\n",
            "üìä Retrieved 3 contextual chunks:\n",
            "\n",
            "üèÜ Chunk #1 (Similarity: 0.7855)\n",
            "üìù Section: **Uniswap Principles | Uni-code**\n",
            "üìÑ Content: ## **Uniswap Principles | Uni-code**\n",
            "\n",
            "*Our Uniswap operating principles (Unicode) articulate who we are (our values) and how we work. They are our daily guideposts for how we interact with each other,...\n",
            "üß† Context: This section on \"Uniswap Principles | Uni-code\" outlines Uniswap's core values and guiding principles that shape the company's culture and how employees interact with each other, users, and the broader community. These principles serve as daily guideposts for Uniswap's performance-driven and collaborative work environment, and are foundational to the company's mission of building a people-first, internet-native financial system.\n",
            "\n",
            "üèÜ Chunk #2 (Similarity: 0.6482)\n",
            "üìù Section: Code of Ethics {#code-of-ethics}\n",
            "üìÑ Content: ## Code of Ethics {#code-of-ethics}\n",
            "\n",
            "Uniswap Labs‚Äô Code of Ethics is one of the ways we put our UNIcode values into practice. We empower all team members to have their own personal standards and make ...\n",
            "üß† Context: This section on Uniswap's Code of Ethics is part of the \"How We Keep You Safe\" section of the employee handbook, which outlines policies and guidelines related to workplace conduct, ethics, and compliance. It establishes the company's expectations for ethical behavior and professional conduct from all team members, and the consequences for violating these standards.\n",
            "\n",
            "üèÜ Chunk #3 (Similarity: 0.6094)\n",
            "üìù Section: Social Media Guidelines  {#social-media-guidelines}\n",
            "üìÑ Content: ## Social Media Guidelines  {#social-media-guidelines}\n",
            "\n",
            "It is important to us that we maintain healthy relationships not just internally, but also externally with the Uniswap Labs community. One of ou...\n",
            "üß† Context: This section on Social Media Guidelines is part of Uniswap's overall employee handbook, which covers company policies, employment guidelines, code of conduct, workplace policies, compensation, benefits, and operational procedures. It provides guidance to Uniswap employees on how to responsibly engage with the company's social media platforms and their own personal accounts, with a focus on maintaining the company's reputation and brand.\n",
            "\n",
            "‚úÖ Contextual RAG pipeline completed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"QUERY TO ANSWER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_query = \"What are Uniswap's core values?\"\n",
        "\n",
        "print(f\"\\nQuery: {test_query}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "answer = answer_query_contextual(test_query, contextual_handbook_db)\n",
        "print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"UNDERLYING RETRIEVAL DETAILS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Also show what chunks were retrieved for transparency\n",
        "results = contextual_handbook_db.search(test_query, k=3)\n",
        "print(f\"\\nüìä Retrieved {len(results)} contextual chunks:\")\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"\\nüèÜ Chunk #{i} (Similarity: {result['similarity']:.4f})\")\n",
        "    print(f\"üìù Section: {result['metadata'].get('heading', 'N/A')}\")\n",
        "    print(f\"üìÑ Content: {result['metadata']['original_content'][:200]}...\")\n",
        "    print(f\"üß† Context: {result['metadata']['contextual_content']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Contextual RAG pipeline completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìñ Loading contextual employee handbook database...\n",
            "\n",
            "üîç Testing RAG with contextual embeddings:\n",
            "--------------------------------------------------\n",
            "\n",
            "1. Query: What are Uniswap's core values?\n",
            "[{'metadata': {'doc_id': 'employee_handbook_contextual', 'original_uuid': 'employee_handbook_contextual_doc', 'chunk_id': 'chunk_1', 'original_index': 1, 'original_content': '## **Uniswap Principles | Uni-code**\\n\\n*Our Uniswap operating principles (Unicode) articulate who we are (our values) and how we work. They are our daily guideposts for how we interact with each other, our users, and our community.*\\n\\n**People First**\\n\\nWe believe easy, safe, fair value transfer on the internet can improve people‚Äôs lives. Access, security and experience is the center of everything we do. By pursuing decentralization, interoperability, and durability we align with our users over the long-term, and win. Internally, people are our greatest asset, and we strive for an environment where everyone can make an incredible impact. We share direct, kind feedback so we can improve. When we advocate for an idea, a technical tradeoff, or a business goal, we start with why ‚Äî why it‚Äôs better for our user, our company.\\n\\n**Simple**\\n\\nOur craft is keeping it simple. In a complex field, we create clarity & simplicity. We write and build in human terms, understood by everyone. This shows up in the design and language of our user experience, across our code, and in how we speak to the world and each other. We are deliberate in the words and images we use.\\n\\n**Pink**\\n\\nWe love pink, conceptually and irl. Disrupting the status quo is fun. We hold each other accountable to do serious work ‚Äì without taking ourselves too seriously. We love unicorns, and we bring whimsy into our experiments and our day-to-day.  The people-first, internet-native financial system we‚Äôre building welcomes anyone.\\n\\n**Push Through Ambiguity Together**\\n\\nStartup life can be ambiguous ‚Äî and especially at the frontier. We are at peace with the uncertainty. We push through resistance. We show up ready to explore new ideas. We have conviction that our mission is worth it. When things get hard, we stay on the same team. We create scalable solutions to make things less ambiguous in pursuit of our mission. We don‚Äôt assume shared knowledge or beliefs across our team or our users, and we seek to understand others‚Äô ‚Äúwhy.‚Äù When we don‚Äôt agree with something, we assume positive intent & default to trust.\\n\\n**Build to last, iterate fast**\\n\\nWe see through the clouds of today into what‚Äôs possible tomorrow. We don‚Äôt fall prey to short-term thinking or get distracted by narratives and prices. We aim to move fast, learn from our mistakes, and we ship when it‚Äôs right. Where our opinions bring safety, simplicity, and new choices in the best interest of our users, we express them. This long-term view is a big part of why we are a beloved brand. We must protect it.\\n\\n| How We Keep You Safe |\\n| :---: |\\n\\n![][image2]', 'contextual_content': 'This section on \"Uniswap Principles | Uni-code\" outlines Uniswap\\'s core values and guiding principles that shape the company\\'s culture and how employees interact with each other, users, and the broader community. These principles serve as daily guideposts for Uniswap\\'s performance-driven and collaborative work environment, and are foundational to the company\\'s mission of building a people-first, internet-native financial system.', 'heading': '**Uniswap Principles | Uni-code**', 'link': ''}, 'similarity': 0.7854784457586674}, {'metadata': {'doc_id': 'employee_handbook_contextual', 'original_uuid': 'employee_handbook_contextual_doc', 'chunk_id': 'chunk_2', 'original_index': 2, 'original_content': '## Code of Ethics {#code-of-ethics}\\n\\nUniswap Labs‚Äô Code of Ethics is one of the ways we put our UNIcode values into practice. We empower all team members to have their own personal standards and make decisions that impact our reputation. However, individual actions at work shape Uniswap Labs‚Äô internal environment and external reputation, which is why it is important to take responsibility to act with respect, honesty, cooperation and fairness in all situations. Whenever you‚Äôre faced with a decision, always do what you know is ethically right, and, of course, always follow the law.\\n\\nWe expect that team members will not knowingly misrepresent Uniswap Labs and will not speak on behalf of Uniswap Labs unless specifically authorized. The confidentiality of proprietary information, and confidential or commercially-sensitive information (i.e. business strategies, business initiatives, business relationships or business, legal or financial affairs etc.) which we haven‚Äôt released to the public or is not generally known about Uniswap Labs, or that of our customers or partners (i.e., ‚Äútrade secret‚Äù), is to be treated with discretion and only disseminated on a need-to-know basis.\\n\\nWe strive to improve the quality of our services, products, and operations and will maintain a reputation for honesty, fairness, respect, responsibility, integrity, trust, and sound business judgment.\\n\\nViolation of the Code of Ethics can result in discipline, up to and including termination of employment. The degree of discipline imposed may be influenced by the existence of voluntary disclosure of any ethical violation and whether or not the violator cooperated in any subsequent investigation.\\n\\nAs a condition of employment, all team members are required to sign and abide by the *Universal Navigation Inc. Confidential Information and Invention Assignment Agreement (CIIAA)*, including the confidentiality/nondisclosure provisions of that agreement.', 'contextual_content': 'This section on Uniswap\\'s Code of Ethics is part of the \"How We Keep You Safe\" section of the employee handbook, which outlines policies and guidelines related to workplace conduct, ethics, and compliance. It establishes the company\\'s expectations for ethical behavior and professional conduct from all team members, and the consequences for violating these standards.', 'heading': 'Code of Ethics {#code-of-ethics}', 'link': ''}, 'similarity': 0.6481045870564754}, {'metadata': {'doc_id': 'employee_handbook_contextual', 'original_uuid': 'employee_handbook_contextual_doc', 'chunk_id': 'chunk_7', 'original_index': 7, 'original_content': '## Social Media Guidelines  {#social-media-guidelines}\\n\\nIt is important to us that we maintain healthy relationships not just internally, but also externally with the Uniswap Labs community. One of our favorite ways to engage is through our social media platforms, and we have some guidance for how best to do that outlined below. \\n\\nUniswap Labs‚Äô Official Accounts  \\nWe have official Uniswap Labs accounts on [Discord](https://discord.com/invite/FCfyBSbCU5), [Twitter](https://twitter.com/Uniswap), and [Github](https://github.com/Uniswap), all of which are easily found through our [website](https://uniswap.org/). Familiarize yourself with the Uniswap Labs brand and audiences; it is a great way to build relationships with consumers. We encourage you to follow  us on any and all of these platforms to stay up to date & informed. \\n\\nPersonal Social Media Accounts  \\nShould you choose to engage in social media via your personal accounts, please use your best judgment when deciding what to post. Remember that an employee of Uniswap Labs, can be interpreted as representing the company. If you have something you want to post about the markets, your experience at work in the general sense, your thoughts on crypto, might be interpreted as coming by the Uniswap Labs team, please run it by the communications team before posting.\\n\\nGuidelines:\\n\\n* We expect that all Uniswap Labs team members will protect proprietary, confidential and copyrighted information, as well as refrain from posting derogatory content.  \\n* Avoid using your social accounts to pump your bags ‚Äì shilling tokens or projects for your own gain doesn‚Äôt represent you or Uniswap Labs in an ethical or reputable way. If you are using applications that distribute tokens and are tied to Twitter or other social accounts, we encourage you to not include any affiliation to Uniswap.   \\n* Be a crypto and web3 community evangelist \\\\- avoid spreading FUD or being a price puppeteer.   \\n* Feel free to show your creativity or rep a favorite artist by using NFTs for your profile image.  \\n* Consider the images and personal life details you share \\\\- drug usage, drug paraphernalia, discriminatory or hate speech, disrespectful statements and/or other similar content will be on the internet and follow you forever, and will be interpreted by some as a representation of the UL brand in a negative light. Some posts could also violate this policy so be smart about what you share.\\n\\nPotential clients and partners will want to get to know the organization and sometimes even team members; always present your best and true self. We will not continuously monitor your personal accounts, however all content you post should be done responsibly. We reserve the right to discipline team members in accordance with our policies for any content posted that is deemed inappropriate. \\n\\nWhen talking about the Protocol, you **must** follow these guidelines (on Social Media and/or informal settings):\\n\\n* Feel free to discuss the protocol generally: what it is, how it works, how it originated, what makes it unique, how it is better than any other protocol or AMM or the legacy financial system, etc.\\n\\n* Feel free to discuss the vibrant community, many integrations, and varied ecosystem contributions related to the Uniswap protocol. \\n\\nWays you **should not** talk about the Uniswap protocol, Uniswap Labs, or the UNI token (on Social Media and/or informal settings):\\n\\n* Do not use ‚ÄúUniswap‚Äù as a stand alone term or shorthand wherever possible. Always refer to ‚ÄúUniswap Labs‚Äù, the ‚ÄúUniswap team‚Äù, the ‚ÄúUniswap protocol‚Äù, ‚ÄúUniswap interface‚Äù, etc.  \\n\\n* Do not use ‚Äúwe‚Äù to refer to the protocol (e.g., don‚Äôt say ‚Äúwe do 1B in daily volume‚Äù). If you are talking about the team or the interface, use ‚ÄúUniswap Labs‚Äù or ‚ÄúUniswap interface‚Äù. If you mean the protocol, say ‚ÄúUniswap protocol‚Äù. \\n\\n* Do not highlight the ability to do a token sale through the Uniswap protocol.\\n\\n* Do not mention the price of anything ever, expressly or implicitly. \\n\\n  * **Do not mention the UNI token price ever**\\\\--not even implicit references to increases (e.g., don‚Äôt tweet ‚ÄúWhat an amazing day\\\\!‚Äù after UNI has a big jump). \\n\\n  * If you‚Äôre ever asked about UNI, you can say Uniswap Labs does not speculate on the future price of UNI.\\n\\n  * Do not talk about the price of other tokens. The exception here is you can talk about the price of Bitcoin, ETH, or the very general token market (‚Äúbull‚Äù or ‚Äúbear‚Äù etc.), as long as you‚Äôre not using one of them as a proxy for UNI (e.g., don‚Äôt say ‚ÄúUNI is the biggest app on Ethereum and ETH is up today‚Ä¶‚Äù).\\n\\n  * You can talk about the price of Unisocks. Just don‚Äôt suggest it might go up from the current price.\\n\\n* Do not encourage people to buy and sell cryptocurrency (e.g., ‚Äúhodl\\\\!‚Äù ‚Äúbuy the dip\\\\!‚Äù).\\n\\n  * Avoid anything that could look like investment advice.\\n\\n* Generally do not publicly engage in conversations regarding the protocol fee switch that may accrue to UNI tokens.\\n\\n  * Do not say anything that might imply the price of UNI may go up, that we want it to go up, or are happy when it goes up. \\n\\n* Do not suggest that anyone should expect an airdrop. \\n\\n* Do not encourage anyone‚Äôs expectations that people should rely on Uniswap Labs‚Äôs efforts to make the protocol succeed and especially not to make the UNI token valuable. Highlight community efforts so people don‚Äôt incorrectly get the impression that they should rely on us exclusively to drive any value to the UNI token.\\n\\n* Generally try not to reference competitors such as Pancakeswap or Sushiswap because it‚Äôs free publicity for them.', 'contextual_content': \"This section on Social Media Guidelines is part of Uniswap's overall employee handbook, which covers company policies, employment guidelines, code of conduct, workplace policies, compensation, benefits, and operational procedures. It provides guidance to Uniswap employees on how to responsibly engage with the company's social media platforms and their own personal accounts, with a focus on maintaining the company's reputation and brand.\", 'heading': 'Social Media Guidelines  {#social-media-guidelines}', 'link': ''}, 'similarity': 0.609331941404167}]\n",
            "   üìä Found 3 relevant chunks\n",
            "   üéØ Top similarity score: 0.7855\n",
            "   üìù Most relevant section: **Uniswap Principles | Uni-code**...\n"
          ]
        }
      ],
      "source": [
        "# Example: Using the contextual vector database for RAG\n",
        "from rag_operations import answer_query_contextual\n",
        "\n",
        "# Load the contextual database\n",
        "print(\"üìñ Loading contextual employee handbook database...\")\n",
        "contextual_handbook_db = PreComputedContextualVectorDB(\"employee_handbook_contextual\")\n",
        "contextual_handbook_db.load_db()\n",
        "\n",
        "# Test queries\n",
        "test_query = [\n",
        "    \"What are Uniswap's core values?\"\n",
        "]\n",
        "\n",
        "print(\"\\nüîç Testing RAG with contextual embeddings:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, query in enumerate(test_query, 1):\n",
        "    print(f\"\\n{i}. Query: {query}\")\n",
        "    \n",
        "    # Search for relevant chunks\n",
        "    results = contextual_handbook_db.search(query, k=3)\n",
        "    print(f\"   üìä Found {len(results)} relevant chunks\")\n",
        "    print(f\"   üéØ Top similarity score: {results[0]['similarity']:.4f}\")\n",
        "    print(f\"   üìù Most relevant section: {results[0]['metadata'].get('heading', 'N/A')[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combined Contextual RAG (creating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 5 benefits chunks\n",
            "Loaded 77 handbook chunks\n",
            "‚úÖ Created combined dataset with 82 total chunks\n",
            "   - Benefits & Wellbeing: 5 chunks\n",
            "   - Employee Handbook: 77 chunks\n",
            "‚úÖ Saved to: data/combined_documents_with_context.json\n"
          ]
        }
      ],
      "source": [
        "## Creating a Combined VectorDB with Multiple Documents\n",
        "\n",
        "# Let's create a combined dataset that includes both benefits_wellbeing and employee_handbook\n",
        "def create_combined_dataset():\n",
        "    \"\"\"\n",
        "    Create a combined dataset with both benefits_wellbeing and employee_handbook documents\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load both JSON files\n",
        "    with open('data/benefits_wellbeing_with_context.json', 'r') as f:\n",
        "        benefits_data_context = json.load(f)\n",
        "    \n",
        "    with open('data/employee_handbook_with_context.json', 'r') as f:\n",
        "        handbook_data_context = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded {len(benefits_data_context)} benefits chunks\")\n",
        "    print(f\"Loaded {len(handbook_data_context)} handbook chunks\")\n",
        "    \n",
        "    def transform_to_combined_format(raw_data, doc_id, doc_type):\n",
        "        \"\"\"Transform data to VectorDB format with document identification\"\"\"\n",
        "        doc = {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"original_uuid\": f\"{doc_id}_uuid\",\n",
        "            \"doc_type\": doc_type,  # Add document type for easier filtering\n",
        "            \"chunks\": []\n",
        "        }\n",
        "        \n",
        "        for i, item in enumerate(raw_data):\n",
        "            chunk = {\n",
        "                \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
        "                \"original_index\": i,\n",
        "                \"content\": item[\"text\"],\n",
        "                \"heading\": item[\"chunk_heading\"],\n",
        "                \"link\": item[\"chunk_link\"],\n",
        "                \"doc_type\": doc_type,  # Also add to chunk metadata\n",
        "                \"source_doc\": doc_id  # Clear source identification\n",
        "            }\n",
        "            doc[\"chunks\"].append(chunk)\n",
        "        \n",
        "        return doc\n",
        "    \n",
        "    # Transform both datasets\n",
        "    benefits_doc = transform_to_combined_format(benefits_data_context, \"benefits_wellbeing\", \"benefits\")\n",
        "    handbook_doc = transform_to_combined_format(handbook_data_context, \"employee_handbook\", \"handbook\")\n",
        "    \n",
        "    # Combine into a single dataset\n",
        "    combined_contextual_dataset = [benefits_doc, handbook_doc]\n",
        "    \n",
        "    # Save the combined dataset\n",
        "    output_file = 'data/combined_documents_with_context.json'\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(combined_contextual_dataset, f, indent=2)\n",
        "    \n",
        "    total_chunks = len(benefits_doc[\"chunks\"]) + len(handbook_doc[\"chunks\"])\n",
        "    print(f\"‚úÖ Created combined dataset with {total_chunks} total chunks\")\n",
        "    print(f\"   - Benefits & Wellbeing: {len(benefits_doc['chunks'])} chunks\")\n",
        "    print(f\"   - Employee Handbook: {len(handbook_doc['chunks'])} chunks\")\n",
        "    print(f\"‚úÖ Saved to: {output_file}\")\n",
        "    \n",
        "    return combined_contextual_dataset\n",
        "\n",
        "# Create the combined dataset\n",
        "combined_contextual_dataset = create_combined_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating combined VectorDB with both documents...\n",
            "Loading vector database from disk.\n",
            "‚úÖ Combined VectorDB ready! Total chunks: 82\n"
          ]
        }
      ],
      "source": [
        "# Initialize a combined VectorDB\n",
        "print(\"Creating combined VectorDB with both documents...\")\n",
        "\n",
        "# Initialize the VectorDB for combined documents\n",
        "combined_db = VectorDB(\"combined_documents_with_context\")\n",
        "combined_db.load_data(combined_contextual_dataset)\n",
        "\n",
        "total_chunks = sum(len(doc['chunks']) for doc in combined_contextual_dataset)\n",
        "print(f\"‚úÖ Combined VectorDB ready! Total chunks: {total_chunks}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced RAG Functions for Combined Database\n",
        "\n",
        "def retrieve_combined(query, combined_db, k=5):\n",
        "    \"\"\"Retrieve relevant documents from combined database with source information\"\"\"\n",
        "    results = combined_db.search(query, k=k)\n",
        "    context = \"\"\n",
        "    sources = []\n",
        "    \n",
        "    for result in results:\n",
        "        chunk = result['metadata']\n",
        "        \n",
        "        # Extract source information - handle the actual metadata structure\n",
        "        doc_id = chunk.get('doc_id', 'unknown_doc')\n",
        "        chunk_id = chunk.get('chunk_id', 'unknown_chunk')\n",
        "        \n",
        "        # Determine source document from doc_id\n",
        "        if 'benefits' in doc_id.lower():\n",
        "            source_doc = 'benefits_wellbeing'\n",
        "        elif 'handbook' in doc_id.lower() or 'employee' in doc_id.lower():\n",
        "            source_doc = 'employee_handbook'\n",
        "        else:\n",
        "            source_doc = doc_id\n",
        "        \n",
        "        # Try to extract heading from content (first line if it starts with #)\n",
        "        content = chunk.get('content', '')\n",
        "        heading = 'Unknown Section'\n",
        "        content_lines = content.split('\\n')\n",
        "        for line in content_lines[:3]:  # Check first 3 lines\n",
        "            if line.strip().startswith('#'):\n",
        "                heading = line.strip().replace('#', '').strip()\n",
        "                break\n",
        "        \n",
        "        # If no heading found, use chunk_id as fallback\n",
        "        if heading == 'Unknown Section':\n",
        "            heading = chunk_id\n",
        "        \n",
        "        # Include source information in context\n",
        "        source_info = f\"[Source: {source_doc} - {heading}]\"\n",
        "        context += f\"\\n{source_info}\\n{content}\\n\"\n",
        "        sources.append({\n",
        "            'source_doc': source_doc,\n",
        "            'heading': heading,\n",
        "            'similarity': result['similarity']\n",
        "        })\n",
        "    \n",
        "    return results, context, sources\n",
        "\n",
        "def answer_query_combined(query, combined_db):\n",
        "    \"\"\"Answer a query using the Combined RAG pipeline\"\"\"\n",
        "    documents, context, sources = retrieve_combined(query, combined_db)\n",
        "    \n",
        "    # Create source summary\n",
        "    source_summary = \"Sources consulted:\\n\"\n",
        "    for source in sources:\n",
        "        source_summary += f\"‚Ä¢ {source['source_doc']}: {source['heading']} (similarity: {source['similarity']:.3f})\\n\"\n",
        "    \n",
        "    prompt = f\"\"\" ### SYSTEM ###\n",
        "    You are **Uniswap Employee Assistant**.\n",
        "\n",
        "    You have been provided with relevant company documents from multiple sources to answer employee questions.\n",
        "\n",
        "    **Workflow for this query:**\n",
        "    1. **Analyze the user question**: {query}\n",
        "    2. **Review the provided context** below for relevant information\n",
        "    3. **Write a natural-language answer** following these rules:\n",
        "    ‚Ä¢ Use only facts that appear verbatim in the provided context\n",
        "    ‚Ä¢ If the information isn't in the context, reply: \"I don't have that information in the provided context.\"\n",
        "    ‚Ä¢ Compose the most **expansive, detailed answer possible** by weaving together **every relevant fact** found in the context‚Äîrephrasing, grouping, and elaborating on those facts for clarity and flow\n",
        "    ‚Ä¢ You may explain terms, list related details, and provide a logical structure\n",
        "    ‚Ä¢ **Never introduce information that is not stated verbatim in the context**\n",
        "    ‚Ä¢ When referencing information, mention which document type it comes from (e.g., \"According to the benefits documentation...\" or \"The employee handbook states...\")\n",
        "    4. **Do not** provide preamble such as \"Here is the answer\" or \"Based on the documents\"\n",
        "    5. **Always append exactly**: \"Double-check with Julian or Megan for any of this information!\"\n",
        "\n",
        "    ### USER QUESTION ###\n",
        "    {query}\n",
        "\n",
        "    ### CONTEXT ###\n",
        "    {context}\n",
        "\n",
        "    ### RESPONSE ###\"\"\"\n",
        "    \n",
        "    response = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=500,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    \n",
        "    # Handle the response content properly\n",
        "    try:\n",
        "        answer = response.content[0].text\n",
        "    except AttributeError:\n",
        "        answer = str(response.content[0])\n",
        "    \n",
        "    return answer, source_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combined Contextual RAG (Prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Tell me about the company's gym benefits\n",
            "\n",
            "Sources consulted:\n",
            "‚Ä¢ employee_handbook: Employee Benefits {employee-benefits} (similarity: 0.470)\n",
            "‚Ä¢ benefits_wellbeing: Health Benefits (similarity: 0.424)\n",
            "‚Ä¢ benefits_wellbeing: 401k & Financial Benefits (similarity: 0.393)\n",
            "‚Ä¢ employee_handbook: Phones (similarity: 0.356)\n",
            "‚Ä¢ employee_handbook: Reasonable Accommodations:  Disability, Nursing Mothers and Religious  {reasonable-accommodations:-disability,-nursing-mothers-and-religious} (similarity: 0.339)\n",
            "\n",
            "Combined Answer: According to the provided company documents, I don't have information about specific gym benefits in the provided context. \n",
            "\n",
            "The employee handbook and benefits documentation cover various health and wellbeing benefits including medical coverage through Anthem Blue Cross and Kaiser, dental plans through Guardian, vision insurance through VSP, One Medical access, and Maven for fertility and family planning. The benefits documentation also mentions that \"Uniswap offers a variety of health & wellbeing benefits to make sure you always feel your best.\"\n",
            "\n",
            "For team members working from home, the employee handbook states that Uniswap Labs reimburses up to $2,000 USD to cover the purchase of office supplies, productivity items, and anything else needed for home office setup. Additionally, for those who prefer working from a co-working space, the company reimburses the cost up to $500 USD per month.\n",
            "\n",
            "However, the documents don't contain any specific information about gym memberships, fitness center access, or gym-related reimbursements as part of the company's benefits package.\n",
            "\n",
            "Double-check with Julian or Megan for any of this information!\n"
          ]
        }
      ],
      "source": [
        "# Test the combined RAG system with a query that might span both documents\n",
        "test_query = \"Tell me about the company's gym benefits\"\n",
        "answer, sources = answer_query_combined(test_query, combined_db)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"\\n{sources}\")\n",
        "print(f\"Combined Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìñ Loading Employee Handbook data for contextual RAG...\n",
            "üîÑ Creating contextual embeddings for Employee Handbook...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/Employee Handbook.md'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m DOCUMENT_NAME = \u001b[33m'\u001b[39m\u001b[33memployee_handbook\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# or 'employee_handbook' , 'benefits_wellbeing'\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Setup contextual embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m benefits_raw, enhanced_chunks = \u001b[43msetup_contextual_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOCUMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Create contextual dataset (includes full document content for context generation)\u001b[39;00m\n\u001b[32m     18\u001b[39m contextual_dataset = create_contextual_dataset()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/anthropic-local-test/contextual_retrieval/main/data_utils.py:447\u001b[39m, in \u001b[36msetup_contextual_embeddings\u001b[39m\u001b[34m(document_name)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(output_file):\n\u001b[32m    445\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ Creating contextual embeddings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mdoc_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     enhanced_chunks = \u001b[43madd_contextual_information_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_markdown_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmarkdown_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdoc_type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdoc_description\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Created contextual embeddings with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(enhanced_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/anthropic-local-test/contextual_retrieval/main/data_utils.py:318\u001b[39m, in \u001b[36madd_contextual_information_general\u001b[39m\u001b[34m(input_file, output_file, original_markdown_file, doc_type, doc_description)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    316\u001b[39m     chunks = json.load(f)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moriginal_markdown_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    319\u001b[39m     full_doc_content = f.read()\n\u001b[32m    321\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/Employee Handbook.md'"
          ]
        }
      ],
      "source": [
        "# Force reload the module to pick up new functions\n",
        "import importlib\n",
        "import data_utils\n",
        "importlib.reload(data_utils)\n",
        "\n",
        "# Import the setup function from data_utils\n",
        "from data_utils import setup_contextual_embeddings\n",
        "\n",
        "# =====================================\n",
        "# CHANGE DOCUMENT HERE - JUST ONE LINE!\n",
        "# =====================================\n",
        "DOCUMENT_NAME = 'employee_handbook'  # or 'employee_handbook' , 'benefits_wellbeing'\n",
        "\n",
        "# Setup contextual embeddings\n",
        "benefits_raw, enhanced_chunks = setup_contextual_embeddings(DOCUMENT_NAME)\n",
        "\n",
        "# Create contextual dataset (includes full document content for context generation)\n",
        "contextual_dataset = create_contextual_dataset()\n",
        "\n",
        "print(f\"Created contextual dataset with {len(contextual_dataset[0]['chunks'])} chunks\")\n",
        "print(f\"Full document length: {len(contextual_dataset[0]['content'])} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
