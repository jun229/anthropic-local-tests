{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac305a3e",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf21757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from supabase import create_client\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9e08c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e068a",
   "metadata": {},
   "source": [
    "### Embed Benefits Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_benefits_chunks():\n",
    "    \"\"\"\n",
    "    Embed the first 3 chunks from benefits_wellbeing_with_context.json \n",
    "    into Supabase test_chunks table (only if they don't already exist)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize clients\n",
    "    print(\"Initializing clients...\")\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    # Check what chunks already exist\n",
    "    print(\"Checking for existing chunks...\")\n",
    "    try:\n",
    "        existing_chunks = supabase.table(\"test_chunks\").select(\"source_file, chunk_index, chunk_heading\").execute()\n",
    "        existing_set = set()\n",
    "        for chunk in existing_chunks.data:\n",
    "            key = (chunk['source_file'], chunk['chunk_index'])\n",
    "            existing_set.add(key)\n",
    "            print(f\"  📋 Found existing: {chunk['chunk_heading']} (index {chunk['chunk_index']})\")\n",
    "        \n",
    "        print(f\"📊 Found {len(existing_chunks.data)} existing chunks in database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking existing chunks: {e}\")\n",
    "        existing_set = set()\n",
    "    \n",
    "    # Load the benefits data\n",
    "    print(\"\\nLoading benefits data...\")\n",
    "    with open(\"data/benefits_wellbeing_with_context.json\", \"r\") as f:\n",
    "        benefits_data = json.load(f)\n",
    "    \n",
    "    # Take first 3 chunks for testing\n",
    "    test_chunks = benefits_data[:3]\n",
    "    print(f\"📊 Processing {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    # Track what we actually process\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(test_chunks):\n",
    "        source_file = \"benefits_wellbeing_with_context.json\"\n",
    "        chunk_key = (source_file, i)\n",
    "        \n",
    "        print(f\"\\nProcessing chunk {i+1}: {chunk['chunk_heading']}\")\n",
    "        \n",
    "        # Check if this chunk already exists\n",
    "        if chunk_key in existing_set:\n",
    "            print(f\"Skipping - chunk already exists in database\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Prepare content for embedding (combine heading + text for better context)\n",
    "        embedding_content = f\"{chunk['chunk_heading']}\\n\\n{chunk['text']}\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        print(f\"🧠 Generating embedding for '{chunk['chunk_heading']}'...\")\n",
    "        try:\n",
    "            response = openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=embedding_content\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            print(f\"✅ Generated embedding with {len(embedding)} dimensions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating embedding: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        chunk_data = {\n",
    "            \"source_file\": source_file,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_heading\": chunk[\"chunk_heading\"],\n",
    "            \"content\": chunk[\"text\"],\n",
    "            \"situational_context\": chunk[\"situational_context\"],\n",
    "            \"embedding\": embedding\n",
    "        }\n",
    "        \n",
    "        # Insert into Supabase\n",
    "        print(f\"💾 Inserting chunk into Supabase...\")\n",
    "        try:\n",
    "            result = supabase.table(\"test_chunks\").insert(chunk_data).execute()\n",
    "            print(f\"✅ Successfully inserted chunk: {chunk['chunk_heading']}\")\n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error inserting into Supabase: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n🎉 Processing complete!\")\n",
    "    print(f\"   ✅ Newly embedded: {processed_count} chunks\")\n",
    "    print(f\"   ⏭️  Skipped existing: {skipped_count} chunks\")\n",
    "    print(f\"   📊 Total chunks: {processed_count + skipped_count}\")\n",
    "    \n",
    "    # Test a simple query\n",
    "    print(\"\\n🔍 Final database state...\")\n",
    "    try:\n",
    "        test_query = supabase.table(\"test_chunks\").select(\"*\").execute()\n",
    "        print(f\"📊 Total chunks in database: {len(test_query.data)}\")\n",
    "        for chunk in test_query.data:\n",
    "            print(f\"  - {chunk['chunk_heading']} (ID: {chunk['id'][:8]}...)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing retrieval: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9223def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_similarity_search(query_text=\"health insurance plans\"):\n",
    "    \"\"\"\n",
    "    Similarity search that handles embedding data types correctly\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Testing similarity search for: '{query_text}'\")\n",
    "    \n",
    "    # Initialize clients\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query\n",
    "        print(\"🧠 Generating query embedding...\")\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query_text\n",
    "        )\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        # Get all chunks\n",
    "        print(\"🔍 Retrieving chunks from database...\")\n",
    "        all_chunks = supabase.table(\"test_chunks\").select(\"*\").execute()\n",
    "        print(f\"📊 Retrieved {len(all_chunks.data)} chunks for similarity comparison\")\n",
    "        \n",
    "        # Debug: Check what type the embedding is\n",
    "        if all_chunks.data:\n",
    "            sample_embedding = all_chunks.data[0]['embedding']\n",
    "            print(f\"🔍 Debug - Embedding type: {type(sample_embedding)}\")\n",
    "            print(f\"🔍 Debug - Embedding preview: {str(sample_embedding)[:100]}...\")\n",
    "        \n",
    "        # Calculate similarities with proper type handling\n",
    "        import numpy as np\n",
    "        similarities = []\n",
    "        \n",
    "        for chunk in all_chunks.data:\n",
    "            if chunk['embedding']:\n",
    "                # Handle different embedding formats from Supabase\n",
    "                chunk_embedding = chunk['embedding']\n",
    "                \n",
    "                # Convert to numpy array if it's a list or string\n",
    "                if isinstance(chunk_embedding, list):\n",
    "                    chunk_embedding = np.array(chunk_embedding)\n",
    "                elif isinstance(chunk_embedding, str):\n",
    "                    # Try parsing as JSON array\n",
    "                    import json\n",
    "                    try:\n",
    "                        chunk_embedding = np.array(json.loads(chunk_embedding))\n",
    "                    except:\n",
    "                        print(f\"❌ Could not parse embedding for {chunk['chunk_heading']}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    # Assume it's already a numpy array or compatible\n",
    "                    chunk_embedding = np.array(chunk_embedding)\n",
    "                \n",
    "                # Ensure query embedding is also numpy array\n",
    "                query_embedding_np = np.array(query_embedding)\n",
    "                \n",
    "                # Calculate cosine similarity (dot product of normalized vectors)\n",
    "                # For now just use dot product for simplicity\n",
    "                similarity = np.dot(query_embedding_np, chunk_embedding)\n",
    "                \n",
    "                similarities.append({\n",
    "                    'chunk': chunk,\n",
    "                    'similarity': float(similarity)  # Ensure it's a regular float\n",
    "                })\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "        print(f\"\\n🎯 Top matches for '{query_text}':\")\n",
    "        for i, match in enumerate(similarities[:3]):\n",
    "            chunk = match['chunk']\n",
    "            score = match['similarity']\n",
    "            print(f\"  {i+1}. {chunk['chunk_heading']} (similarity: {score:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in similarity search: {e}\")\n",
    "        import traceback\n",
    "        print(f\"Full traceback: {traceback.format_exc()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bfb5e9",
   "metadata": {},
   "source": [
    "### Embedding & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60827791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing clients...\n",
      "🔍 Checking for existing chunks...\n",
      "  📋 Found existing: Leaves (index 1)\n",
      "  📋 Found existing: Health Benefits (index 0)\n",
      "  📋 Found existing: Perks (index 2)\n",
      "📊 Found 3 existing chunks in database\n",
      "\n",
      "📂 Loading benefits data...\n",
      "📊 Processing 3 chunks...\n",
      "\n",
      "🔄 Processing chunk 1: Health Benefits\n",
      "⏭️  Skipping - chunk already exists in database\n",
      "\n",
      "🔄 Processing chunk 2: Leaves\n",
      "⏭️  Skipping - chunk already exists in database\n",
      "\n",
      "🔄 Processing chunk 3: Perks\n",
      "⏭️  Skipping - chunk already exists in database\n",
      "\n",
      "🎉 Processing complete!\n",
      "   ✅ Newly embedded: 0 chunks\n",
      "   ⏭️  Skipped existing: 3 chunks\n",
      "   📊 Total chunks: 3\n",
      "\n",
      "🔍 Final database state...\n",
      "📊 Total chunks in database: 3\n",
      "  - Leaves (ID: a64140ab...)\n",
      "  - Health Benefits (ID: fd4e01a2...)\n",
      "  - Perks (ID: d62a02fe...)\n"
     ]
    }
   ],
   "source": [
    "# Test the smart embedding function (will skip existing chunks)\n",
    "embed_benefits_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de6eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing similarity search for: 'health insurance plans'\n",
      "🧠 Generating query embedding...\n",
      "🔍 Retrieving chunks from database...\n",
      "📊 Retrieved 3 chunks for similarity comparison\n",
      "🔍 Debug - Embedding type: <class 'str'>\n",
      "🔍 Debug - Embedding preview: [-0.016314207,0.03778174,0.030891964,0.039434165,-0.038173843,-0.025262512,-0.018246705,-0.004253596...\n",
      "\n",
      "🎯 Top matches for 'health insurance plans':\n",
      "  1. Health Benefits (similarity: 0.489)\n",
      "  2. Leaves (similarity: 0.173)\n",
      "  3. Perks (similarity: 0.137)\n",
      "\n",
      "🔍 Testing similarity search for: 'vacation time off'\n",
      "🧠 Generating query embedding...\n",
      "🔍 Retrieving chunks from database...\n",
      "📊 Retrieved 3 chunks for similarity comparison\n",
      "🔍 Debug - Embedding type: <class 'str'>\n",
      "🔍 Debug - Embedding preview: [-0.016314207,0.03778174,0.030891964,0.039434165,-0.038173843,-0.025262512,-0.018246705,-0.004253596...\n",
      "\n",
      "🎯 Top matches for 'vacation time off':\n",
      "  1. Leaves (similarity: 0.407)\n",
      "  2. Perks (similarity: 0.270)\n",
      "  3. Health Benefits (similarity: 0.200)\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search\n",
    "test_similarity_search(\"health insurance plans\")\n",
    "test_similarity_search(\"vacation time off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
