{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac305a3e",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf21757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from supabase import create_client\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9e08c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e068a",
   "metadata": {},
   "source": [
    "### Embed First 3 Benefits Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_benefits_chunks():\n",
    "    \"\"\"\n",
    "    Embed the first 3 chunks from benefits_wellbeing_with_context.json \n",
    "    into Supabase test_chunks table (only if they don't already exist)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize clients\n",
    "    print(\"Initializing clients...\")\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    # Check what chunks already exist\n",
    "    print(\"Checking for existing chunks...\")\n",
    "    try:\n",
    "        existing_chunks = supabase.table(\"test_chunks\").select(\"source_file, chunk_index, chunk_heading\").execute()\n",
    "        existing_set = set()\n",
    "        for chunk in existing_chunks.data:\n",
    "            key = (chunk['source_file'], chunk['chunk_index'])\n",
    "            existing_set.add(key)\n",
    "            print(f\"  📋 Found existing: {chunk['chunk_heading']} (index {chunk['chunk_index']})\")\n",
    "        \n",
    "        print(f\"📊 Found {len(existing_chunks.data)} existing chunks in database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking existing chunks: {e}\")\n",
    "        existing_set = set()\n",
    "    \n",
    "    # Load the benefits data\n",
    "    print(\"\\nLoading benefits data...\")\n",
    "    with open(\"data/benefits_wellbeing_with_context.json\", \"r\") as f:\n",
    "        benefits_data = json.load(f)\n",
    "    \n",
    "    # Take first 3 chunks for testing\n",
    "    test_chunks = benefits_data[:3]\n",
    "    print(f\"📊 Processing {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    # Track what we actually process\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(test_chunks):\n",
    "        source_file = \"benefits_wellbeing_with_context.json\"\n",
    "        chunk_key = (source_file, i)\n",
    "        \n",
    "        print(f\"\\nProcessing chunk {i+1}: {chunk['chunk_heading']}\")\n",
    "        \n",
    "        # Check if this chunk already exists\n",
    "        if chunk_key in existing_set:\n",
    "            print(f\"Skipping - chunk already exists in database\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Prepare content for embedding (combine heading + text for better context)\n",
    "        embedding_content = f\"{chunk['chunk_heading']}\\n\\n{chunk['text']}\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        print(f\"🧠 Generating embedding for '{chunk['chunk_heading']}'...\")\n",
    "        try:\n",
    "            response = openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=embedding_content\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            print(f\"✅ Generated embedding with {len(embedding)} dimensions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating embedding: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        chunk_data = {\n",
    "            \"source_file\": source_file,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_heading\": chunk[\"chunk_heading\"],\n",
    "            \"content\": chunk[\"text\"],\n",
    "            \"situational_context\": chunk[\"situational_context\"],\n",
    "            \"embedding\": embedding\n",
    "        }\n",
    "        \n",
    "        # Insert into Supabase\n",
    "        print(f\"💾 Inserting chunk into Supabase...\")\n",
    "        try:\n",
    "            result = supabase.table(\"test_chunks\").insert(chunk_data).execute()\n",
    "            print(f\"✅ Successfully inserted chunk: {chunk['chunk_heading']}\")\n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error inserting into Supabase: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n🎉 Processing complete!\")\n",
    "    print(f\"   ✅ Newly embedded: {processed_count} chunks\")\n",
    "    print(f\"   ⏭️  Skipped existing: {skipped_count} chunks\")\n",
    "    print(f\"   📊 Total chunks: {processed_count + skipped_count}\")\n",
    "    \n",
    "    # Test a simple query\n",
    "    print(\"\\n🔍 Final database state...\")\n",
    "    try:\n",
    "        test_query = supabase.table(\"test_chunks\").select(\"*\").execute()\n",
    "        print(f\"📊 Total chunks in database: {len(test_query.data)}\")\n",
    "        for chunk in test_query.data:\n",
    "            print(f\"  - {chunk['chunk_heading']} (ID: {chunk['id'][:8]}...)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing retrieval: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9223def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_similarity_search(query_text=\"health insurance plans\"):\n",
    "    \"\"\"\n",
    "    Similarity search that handles embedding data types correctly\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Testing similarity search for: '{query_text}'\")\n",
    "    \n",
    "    # Initialize clients\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query\n",
    "        print(\"🧠 Generating query embedding...\")\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query_text\n",
    "        )\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        # Get all chunks\n",
    "        print(\"🔍 Retrieving chunks from database...\")\n",
    "        all_chunks = supabase.table(\"test_chunks\").select(\"*\").execute()\n",
    "        print(f\"📊 Retrieved {len(all_chunks.data)} chunks for similarity comparison\")\n",
    "        \n",
    "        # Debug: Check what type the embedding is\n",
    "        if all_chunks.data:\n",
    "            sample_embedding = all_chunks.data[0]['embedding']\n",
    "            print(f\"🔍 Debug - Embedding type: {type(sample_embedding)}\")\n",
    "            print(f\"🔍 Debug - Embedding preview: {str(sample_embedding)[:100]}...\")\n",
    "        \n",
    "        # Calculate similarities with proper type handling\n",
    "        import numpy as np\n",
    "        similarities = []\n",
    "        \n",
    "        for chunk in all_chunks.data:\n",
    "            if chunk['embedding']:\n",
    "                # Handle different embedding formats from Supabase\n",
    "                chunk_embedding = chunk['embedding']\n",
    "                \n",
    "                # Convert to numpy array if it's a list or string\n",
    "                if isinstance(chunk_embedding, list):\n",
    "                    chunk_embedding = np.array(chunk_embedding)\n",
    "                elif isinstance(chunk_embedding, str):\n",
    "                    # Try parsing as JSON array\n",
    "                    import json\n",
    "                    try:\n",
    "                        chunk_embedding = np.array(json.loads(chunk_embedding))\n",
    "                    except:\n",
    "                        print(f\"❌ Could not parse embedding for {chunk['chunk_heading']}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    # Assume it's already a numpy array or compatible\n",
    "                    chunk_embedding = np.array(chunk_embedding)\n",
    "                \n",
    "                # Ensure query embedding is also numpy array\n",
    "                query_embedding_np = np.array(query_embedding)\n",
    "                \n",
    "                # Calculate cosine similarity (dot product of normalized vectors)\n",
    "                # For now just use dot product for simplicity\n",
    "                similarity = np.dot(query_embedding_np, chunk_embedding)\n",
    "                \n",
    "                similarities.append({\n",
    "                    'chunk': chunk,\n",
    "                    'similarity': float(similarity)  # Ensure it's a regular float\n",
    "                })\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "        print(f\"\\n🎯 Top matches for '{query_text}':\")\n",
    "        for i, match in enumerate(similarities[:3]):\n",
    "            chunk = match['chunk']\n",
    "            score = match['similarity']\n",
    "            print(f\"  {i+1}. {chunk['chunk_heading']} (similarity: {score:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in similarity search: {e}\")\n",
    "        import traceback\n",
    "        print(f\"Full traceback: {traceback.format_exc()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bfb5e9",
   "metadata": {},
   "source": [
    "### Embedding & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60827791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing clients...\n",
      "🔍 Checking for existing chunks...\n",
      "  📋 Found existing: Leaves (index 1)\n",
      "  📋 Found existing: Health Benefits (index 0)\n",
      "  📋 Found existing: Perks (index 2)\n",
      "📊 Found 3 existing chunks in database\n",
      "\n",
      "📂 Loading benefits data...\n",
      "📊 Processing 3 chunks...\n",
      "\n",
      "🔄 Processing chunk 1: Health Benefits\n",
      "⏭️  Skipping - chunk already exists in database\n",
      "\n",
      "🔄 Processing chunk 2: Leaves\n",
      "⏭️  Skipping - chunk already exists in database\n",
      "\n",
      "🔄 Processing chunk 3: Perks\n",
      "⏭️  Skipping - chunk already exists in database\n",
      "\n",
      "🎉 Processing complete!\n",
      "   ✅ Newly embedded: 0 chunks\n",
      "   ⏭️  Skipped existing: 3 chunks\n",
      "   📊 Total chunks: 3\n",
      "\n",
      "🔍 Final database state...\n",
      "📊 Total chunks in database: 3\n",
      "  - Leaves (ID: a64140ab...)\n",
      "  - Health Benefits (ID: fd4e01a2...)\n",
      "  - Perks (ID: d62a02fe...)\n"
     ]
    }
   ],
   "source": [
    "# Test the smart embedding function (will skip existing chunks)\n",
    "embed_benefits_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de6eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing similarity search for: 'health insurance plans'\n",
      "🧠 Generating query embedding...\n",
      "🔍 Retrieving chunks from database...\n",
      "📊 Retrieved 3 chunks for similarity comparison\n",
      "🔍 Debug - Embedding type: <class 'str'>\n",
      "🔍 Debug - Embedding preview: [-0.016314207,0.03778174,0.030891964,0.039434165,-0.038173843,-0.025262512,-0.018246705,-0.004253596...\n",
      "\n",
      "🎯 Top matches for 'health insurance plans':\n",
      "  1. Health Benefits (similarity: 0.489)\n",
      "  2. Leaves (similarity: 0.173)\n",
      "  3. Perks (similarity: 0.137)\n",
      "\n",
      "🔍 Testing similarity search for: 'vacation time off'\n",
      "🧠 Generating query embedding...\n",
      "🔍 Retrieving chunks from database...\n",
      "📊 Retrieved 3 chunks for similarity comparison\n",
      "🔍 Debug - Embedding type: <class 'str'>\n",
      "🔍 Debug - Embedding preview: [-0.016314207,0.03778174,0.030891964,0.039434165,-0.038173843,-0.025262512,-0.018246705,-0.004253596...\n",
      "\n",
      "🎯 Top matches for 'vacation time off':\n",
      "  1. Leaves (similarity: 0.407)\n",
      "  2. Perks (similarity: 0.270)\n",
      "  3. Health Benefits (similarity: 0.200)\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search\n",
    "test_similarity_search(\"health insurance plans\")\n",
    "test_similarity_search(\"vacation time off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65f1eb",
   "metadata": {},
   "source": [
    "## Embed Entire Contextual Benefits Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b6eba",
   "metadata": {},
   "source": [
    "## Multi-Document Embedding Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebe626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_multiple_documents(document_files, table_name=\"faq_docs\"):\n",
    "    \"\"\"\n",
    "    Embed multiple JSON documents into Supabase table with duplicate checking\n",
    "    \n",
    "    Args:\n",
    "        document_files: List of file paths to JSON documents\n",
    "        table_name: Supabase table name to insert into\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize clients\n",
    "    print(\"🔧 Initializing clients...\")\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    # Check what chunks already exist across all documents\n",
    "    print(\"🔍 Checking for existing chunks...\")\n",
    "    try:\n",
    "        existing_chunks = supabase.table(table_name).select(\"source_file, chunk_index, chunk_heading\").execute()\n",
    "        existing_set = set()\n",
    "        existing_by_file = {}\n",
    "        \n",
    "        for chunk in existing_chunks.data:\n",
    "            key = (chunk['source_file'], chunk['chunk_index'])\n",
    "            existing_set.add(key)\n",
    "            \n",
    "            # Track by file for reporting\n",
    "            file_name = chunk['source_file']\n",
    "            if file_name not in existing_by_file:\n",
    "                existing_by_file[file_name] = []\n",
    "            existing_by_file[file_name].append(chunk['chunk_heading'])\n",
    "        \n",
    "        print(f\"📊 Found {len(existing_chunks.data)} existing chunks in database\")\n",
    "        for file_name, headings in existing_by_file.items():\n",
    "            print(f\"  📋 {file_name}: {len(headings)} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking existing chunks: {e}\")\n",
    "        existing_set = set()\n",
    "        existing_by_file = {}\n",
    "    \n",
    "    # Process each document file\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for doc_file in document_files:\n",
    "        print(f\"\\n📂 Processing document: {doc_file}\")\n",
    "        \n",
    "        # Load the document data\n",
    "        try:\n",
    "            with open(doc_file, \"r\") as f:\n",
    "                document_data = json.load(f)\n",
    "            print(f\"📊 Loaded {len(document_data)} chunks from {doc_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {doc_file}: {e}\")\n",
    "            total_errors += 1\n",
    "            continue\n",
    "        \n",
    "        # Track progress for this document\n",
    "        doc_processed = 0\n",
    "        doc_skipped = 0\n",
    "        doc_errors = 0\n",
    "        \n",
    "        # Process each chunk in the document\n",
    "        for i, chunk in enumerate(document_data):\n",
    "            source_file = doc_file.split('/')[-1]  # Get just the filename\n",
    "            chunk_key = (source_file, i)\n",
    "            \n",
    "            print(f\"\\n  🔄 Processing chunk {i+1}/{len(document_data)}: {chunk.get('chunk_heading', 'Untitled')}\")\n",
    "            \n",
    "            # Check if this chunk already exists\n",
    "            if chunk_key in existing_set:\n",
    "                print(f\"  ⏭️  Skipping - chunk already exists in database\")\n",
    "                doc_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Validate chunk structure\n",
    "            if 'text' not in chunk:\n",
    "                print(f\"  ❌ Skipping - chunk missing 'text' field\")\n",
    "                doc_errors += 1\n",
    "                continue\n",
    "            \n",
    "            # Prepare content for embedding\n",
    "            heading = chunk.get('chunk_heading', 'Untitled')\n",
    "            content = chunk['text']\n",
    "            embedding_content = f\"{heading}\\n\\n{content}\"\n",
    "            \n",
    "            # Generate embedding\n",
    "            print(f\"  🧠 Generating embedding...\")\n",
    "            try:\n",
    "                response = openai_client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=embedding_content\n",
    "                )\n",
    "                embedding = response.data[0].embedding\n",
    "                print(f\"  ✅ Generated embedding with {len(embedding)} dimensions\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error generating embedding: {e}\")\n",
    "                doc_errors += 1\n",
    "                continue\n",
    "            \n",
    "            # Prepare data for insertion\n",
    "            chunk_data = {\n",
    "                \"source_file\": source_file,\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_heading\": heading,\n",
    "                \"content\": content,\n",
    "                \"situational_context\": chunk.get(\"situational_context\", \"\"),\n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "            \n",
    "            # Insert into Supabase\n",
    "            print(f\"  💾 Inserting chunk into Supabase...\")\n",
    "            try:\n",
    "                result = supabase.table(table_name).insert(chunk_data).execute()\n",
    "                print(f\"  ✅ Successfully inserted: {heading}\")\n",
    "                doc_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error inserting into Supabase: {e}\")\n",
    "                doc_errors += 1\n",
    "                continue\n",
    "        \n",
    "        # Document summary\n",
    "        print(f\"\\n📋 Document '{doc_file}' summary:\")\n",
    "        print(f\"   ✅ Newly embedded: {doc_processed} chunks\")\n",
    "        print(f\"   ⏭️  Skipped existing: {doc_skipped} chunks\")\n",
    "        print(f\"   ❌ Errors: {doc_errors} chunks\")\n",
    "        \n",
    "        # Update totals\n",
    "        total_processed += doc_processed\n",
    "        total_skipped += doc_skipped\n",
    "        total_errors += doc_errors\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n🎉 Multi-document processing complete!\")\n",
    "    print(f\"   📁 Documents processed: {len(document_files)}\")\n",
    "    print(f\"   ✅ Total newly embedded: {total_processed} chunks\")\n",
    "    print(f\"   ⏭️  Total skipped existing: {total_skipped} chunks\")\n",
    "    print(f\"   ❌ Total errors: {total_errors} chunks\")\n",
    "    \n",
    "    # Final database state\n",
    "    print(f\"\\n🔍 Final database state...\")\n",
    "    try:\n",
    "        final_query = supabase.table(table_name).select(\"source_file, chunk_heading\").execute()\n",
    "        print(f\"📊 Total chunks in '{table_name}' table: {len(final_query.data)}\")\n",
    "        \n",
    "        # Group by source file\n",
    "        by_file = {}\n",
    "        for chunk in final_query.data:\n",
    "            file_name = chunk['source_file']\n",
    "            if file_name not in by_file:\n",
    "                by_file[file_name] = []\n",
    "            by_file[file_name].append(chunk['chunk_heading'])\n",
    "        \n",
    "        for file_name, headings in by_file.items():\n",
    "            print(f\"  📋 {file_name}: {len(headings)} chunks\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying final state: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b862de",
   "metadata": {},
   "source": [
    "### Test Multi-Document Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage function\n",
    "def embed_all_available_documents():\n",
    "    \"\"\"\n",
    "    Embed all available documents in the data directory\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # List of documents to process\n",
    "    document_files = [\n",
    "        \"data/benefits_wellbeing_with_context.json\",\n",
    "        \"data/employee_handbook_with_context.json\"\n",
    "    ]\n",
    "    \n",
    "    # Filter to only existing files\n",
    "    existing_files = []\n",
    "    for file_path in document_files:\n",
    "        if os.path.exists(file_path):\n",
    "            existing_files.append(file_path)\n",
    "            print(f\"✅ Found: {file_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Missing: {file_path}\")\n",
    "    \n",
    "    if not existing_files:\n",
    "        print(\"❌ No document files found!\")\n",
    "        return\n",
    "    \n",
    "    # Embed all documents\n",
    "    embed_multiple_documents(existing_files, table_name=\"faq_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fdaa26",
   "metadata": {},
   "source": [
    "## Enhanced Search Using Your match_chunks Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents_knn(query_text, match_count=5, table_name=\"test_chunks\", source_file_filter=None):\n",
    "    \"\"\"\n",
    "    Enhanced search using your match_chunks SQL function with KNN and cosine distance\n",
    "    \n",
    "    Args:\n",
    "        query_text: The search query\n",
    "        match_count: Number of results to return\n",
    "        table_name: Which table to search ('test_chunks' or 'faq_docs')\n",
    "        source_file_filter: Optional - filter to specific document (e.g., 'benefits_wellbeing_with_context.json')\n",
    "    \n",
    "    Returns:\n",
    "        List of matching chunks with similarity scores\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔍 Searching for: '{query_text}'\")\n",
    "    if source_file_filter:\n",
    "        print(f\"📁 Filtering to document: {source_file_filter}\")\n",
    "    \n",
    "    # Initialize clients\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query\n",
    "        print(\"🧠 Generating query embedding...\")\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query_text\n",
    "        )\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        # Use your match_chunks function (modify for different tables if needed)\n",
    "        print(f\"🔍 Searching using KNN with cosine distance...\")\n",
    "        \n",
    "        if table_name == \"test_chunks\":\n",
    "            # Use your existing match_chunks function\n",
    "            result = supabase.rpc('match_chunks', {\n",
    "                'query_embedding': query_embedding,\n",
    "                'match_count': match_count\n",
    "            }).execute()\n",
    "        else:\n",
    "            # For faq_docs, we'll need a similar function or use direct SQL\n",
    "            # For now, let's create a direct query approach\n",
    "            print(f\"⚠️  Using direct query for {table_name} table...\")\n",
    "            \n",
    "            # Direct SQL query for other tables\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                id,\n",
    "                source_file,\n",
    "                chunk_index,\n",
    "                chunk_heading,\n",
    "                content,\n",
    "                situational_context,\n",
    "                1 - (embedding <=> %s) as similarity\n",
    "            FROM {table_name}\n",
    "            WHERE embedding IS NOT NULL\n",
    "            ORDER BY embedding <=> %s\n",
    "            LIMIT %s\n",
    "            \"\"\"\n",
    "            \n",
    "            # Note: This is a simplified approach - you'd want to create match_chunks_faq function\n",
    "            # For now, let's use the existing approach but mention the limitation\n",
    "            result = supabase.table(table_name).select(\"*\").execute()\n",
    "            \n",
    "            # Calculate similarities manually (fallback)\n",
    "            import numpy as np\n",
    "            similarities = []\n",
    "            query_embedding_np = np.array(query_embedding)\n",
    "            \n",
    "            for chunk in result.data:\n",
    "                if chunk['embedding']:\n",
    "                    # Handle different embedding formats\n",
    "                    chunk_embedding = chunk['embedding']\n",
    "                    if isinstance(chunk_embedding, str):\n",
    "                        import json\n",
    "                        try:\n",
    "                            chunk_embedding = np.array(json.loads(chunk_embedding))\n",
    "                        except:\n",
    "                            continue\n",
    "                    else:\n",
    "                        chunk_embedding = np.array(chunk_embedding)\n",
    "                    \n",
    "                    # Calculate cosine similarity (1 - cosine distance)\n",
    "                    cosine_sim = np.dot(query_embedding_np, chunk_embedding) / (\n",
    "                        np.linalg.norm(query_embedding_np) * np.linalg.norm(chunk_embedding)\n",
    "                    )\n",
    "                    \n",
    "                    similarities.append({\n",
    "                        'id': chunk['id'],\n",
    "                        'source_file': chunk['source_file'],\n",
    "                        'chunk_index': chunk['chunk_index'],\n",
    "                        'chunk_heading': chunk['chunk_heading'],\n",
    "                        'content': chunk['content'],\n",
    "                        'situational_context': chunk['situational_context'],\n",
    "                        'similarity': float(cosine_sim)\n",
    "                    })\n",
    "            \n",
    "            # Sort by similarity and limit\n",
    "            similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "            result.data = similarities[:match_count]\n",
    "        \n",
    "        matches = result.data\n",
    "        \n",
    "        # Filter by source file if specified\n",
    "        if source_file_filter:\n",
    "            matches = [m for m in matches if m['source_file'] == source_file_filter]\n",
    "            print(f\"📊 Found {len(matches)} matches in {source_file_filter}\")\n",
    "        else:\n",
    "            print(f\"📊 Found {len(matches)} matches across all documents\")\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n🎯 Top matches for '{query_text}':\")\n",
    "        for i, match in enumerate(matches):\n",
    "            similarity = match['similarity']\n",
    "            heading = match['chunk_heading']\n",
    "            source = match['source_file']\n",
    "            print(f\"  {i+1}. {heading}\")\n",
    "            print(f\"     📁 Source: {source}\")\n",
    "            print(f\"     📊 Similarity: {similarity:.3f}\")\n",
    "            print(f\"     📝 Preview: {match['content'][:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        return matches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in KNN search: {e}\")\n",
    "        import traceback\n",
    "        print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "        return []\n",
    "\n",
    "def search_specific_document(query_text, document_name, match_count=3):\n",
    "    \"\"\"\n",
    "    Search within a specific document only\n",
    "    \"\"\"\n",
    "    return search_documents_knn(\n",
    "        query_text=query_text,\n",
    "        match_count=match_count,\n",
    "        source_file_filter=document_name\n",
    "    )\n",
    "\n",
    "def search_all_documents(query_text, match_count=5):\n",
    "    \"\"\"\n",
    "    Search across all documents\n",
    "    \"\"\"\n",
    "    return search_documents_knn(\n",
    "        query_text=query_text,\n",
    "        match_count=match_count\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d3881",
   "metadata": {},
   "source": [
    "## Test Enhanced KNN Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced KNN search with your match_chunks function\n",
    "print(\"=== Testing KNN Search Across All Documents ===\")\n",
    "search_all_documents(\"health insurance benefits\", match_count=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Testing Search Within Specific Document ===\")\n",
    "search_specific_document(\"vacation time\", \"benefits_wellbeing_with_context.json\", match_count=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741f7bb",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266e7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_all_benefits_chunks():\n",
    "    \"\"\"\n",
    "    Embed all chunks from benefits_wellbeing_with_context.json \n",
    "    into Supabase faq_docs table (only if they don't already exist)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize clients\n",
    "    print(\"Initializing clients...\")\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    # Check what chunks already exist\n",
    "    print(\"Checking for existing chunks...\")\n",
    "    try:\n",
    "        existing_chunks = supabase.table(\"faq_docs\").select(\"source_file, chunk_index, chunk_heading\").execute()\n",
    "        existing_set = set()\n",
    "        for chunk in existing_chunks.data:\n",
    "            key = (chunk['source_file'], chunk['chunk_index'])\n",
    "            existing_set.add(key)\n",
    "            print(f\"  📋 Found existing: {chunk['chunk_heading']} (index {chunk['chunk_index']})\")\n",
    "        \n",
    "        print(f\"📊 Found {len(existing_chunks.data)} existing chunks in database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking existing chunks: {e}\")\n",
    "        existing_set = set()\n",
    "    \n",
    "    # Load the benefits data\n",
    "    print(\"\\nLoading benefits data...\")\n",
    "    with open(\"data/benefits_wellbeing_with_context.json\", \"r\") as f:\n",
    "        benefits_data = json.load(f)\n",
    "\n",
    "    print(f\"📊 Processing {len(benefits_data)} chunks...\")\n",
    "    \n",
    "    # Track what we actually process\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(benefits_data):\n",
    "        source_file = \"benefits_wellbeing_with_context.json\"\n",
    "        chunk_key = (source_file, i)\n",
    "        \n",
    "        print(f\"\\nProcessing chunk {i+1}: {chunk['chunk_heading']}\")\n",
    "        \n",
    "        # Check if this chunk already exists\n",
    "        if chunk_key in existing_set:\n",
    "            print(f\"Skipping - chunk already exists in database\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Prepare content for embedding (combine heading + text for better context)\n",
    "        embedding_content = f\"{chunk['chunk_heading']}\\n\\n{chunk['text']}\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        print(f\"Generating embedding for '{chunk['chunk_heading']}'...\")\n",
    "        try:\n",
    "            response = openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=embedding_content\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            print(f\"Embeddings with: {len(embedding)} dimensions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating embedding: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        chunk_data = {\n",
    "            \"source_file\": source_file,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_heading\": chunk[\"chunk_heading\"],\n",
    "            \"content\": chunk[\"text\"],\n",
    "            \"situational_context\": chunk[\"situational_context\"],\n",
    "            \"embedding\": embedding\n",
    "        }\n",
    "        \n",
    "        # Insert into Supabase\n",
    "        print(f\"💾 Inserting chunk into Supabase...\")\n",
    "        try:\n",
    "            result = supabase.table(\"faq_docs\").insert(chunk_data).execute()\n",
    "            print(f\"✅ Successfully inserted chunk: {chunk['chunk_heading']}\")\n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error inserting into Supabase: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n🎉 Processing complete!\")\n",
    "    print(f\"   ✅ Newly embedded: {processed_count} chunks\")\n",
    "    print(f\"   ⏭️  Skipped existing: {skipped_count} chunks\")\n",
    "    print(f\"   📊 Total chunks: {processed_count + skipped_count}\")\n",
    "    \n",
    "    # Test a simple query\n",
    "    print(\"\\n🔍 Final database state...\")\n",
    "    try:\n",
    "        test_query = supabase.table(\"faq_docs\").select(\"*\").execute()\n",
    "        print(f\"📊 Total chunks in database: {len(test_query.data)}\")\n",
    "        for chunk in test_query.data:\n",
    "            print(f\"  - {chunk['chunk_heading']} (ID: {chunk['id'][:8]}...)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing retrieval: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa32042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing clients...\n",
      "Checking for existing chunks...\n",
      "📊 Found 0 existing chunks in database\n",
      "\n",
      "Loading benefits data...\n",
      "📊 Processing 5 chunks...\n",
      "\n",
      "Processing chunk 1: Health Benefits\n",
      "Generating embedding for 'Health Benefits'...\n",
      "Embeddings with: 1536 dimensions\n",
      "💾 Inserting chunk into Supabase...\n",
      "✅ Successfully inserted chunk: Health Benefits\n",
      "\n",
      "Processing chunk 2: Leaves\n",
      "Generating embedding for 'Leaves'...\n",
      "Embeddings with: 1536 dimensions\n",
      "💾 Inserting chunk into Supabase...\n",
      "✅ Successfully inserted chunk: Leaves\n",
      "\n",
      "Processing chunk 3: Perks\n",
      "Generating embedding for 'Perks'...\n",
      "Embeddings with: 1536 dimensions\n",
      "💾 Inserting chunk into Supabase...\n",
      "✅ Successfully inserted chunk: Perks\n",
      "\n",
      "Processing chunk 4: 401k & Financial Benefits\n",
      "Generating embedding for '401k & Financial Benefits'...\n",
      "Embeddings with: 1536 dimensions\n",
      "💾 Inserting chunk into Supabase...\n",
      "✅ Successfully inserted chunk: 401k & Financial Benefits\n",
      "\n",
      "Processing chunk 5: Time Off &  Holidays\n",
      "Generating embedding for 'Time Off &  Holidays'...\n",
      "Embeddings with: 1536 dimensions\n",
      "💾 Inserting chunk into Supabase...\n",
      "✅ Successfully inserted chunk: Time Off &  Holidays\n",
      "\n",
      "🎉 Processing complete!\n",
      "   ✅ Newly embedded: 5 chunks\n",
      "   ⏭️  Skipped existing: 0 chunks\n",
      "   📊 Total chunks: 5\n",
      "\n",
      "🔍 Final database state...\n",
      "📊 Total chunks in database: 5\n",
      "  - Health Benefits (ID: a8114f3a...)\n",
      "  - Leaves (ID: ed5d0ed1...)\n",
      "  - Perks (ID: 825fce90...)\n",
      "  - 401k & Financial Benefits (ID: f505b922...)\n",
      "  - Time Off &  Holidays (ID: 997a8210...)\n"
     ]
    }
   ],
   "source": [
    "# Embed all chunks\n",
    "embed_all_benefits_chunks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
